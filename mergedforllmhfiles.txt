// src\CommandBufferManager.h
#ifndef COMMANDBUFFERMANAGER_H
#define COMMANDBUFFERMANAGER_H

#include <vulkan/vulkan.h>
#include <mutex>
#include <queue>

class CommandBufferManager {
public:
    CommandBufferManager(VkDevice device, VkCommandPool commandPool);
    ~CommandBufferManager();

    VkCommandBuffer acquireCommandBuffer();
    void releaseCommandBuffer(VkCommandBuffer commandBuffer);

private:
    VkDevice device;
    VkCommandPool commandPool;
    std::mutex mutex;
    std::queue<VkCommandBuffer> commandBuffers;
};

#endif // COMMANDBUFFERMANAGER_H

// src\DescriptorSetManager.h
#ifndef DESCRIPTORSETMANAGER_H
#define DESCRIPTORSETMANAGER_H

#include <vulkan/vulkan.h>
#include <vector>

class DescriptorSetManager {
public:
    DescriptorSetManager(VkDevice device, VkDescriptorPool descriptorPool, VkDescriptorSetLayout descriptorSetLayout);
    ~DescriptorSetManager();

    VkDescriptorSet allocateDescriptorSet();
    void updateDescriptorSet(VkDescriptorSet descriptorSet, const std::vector<VkBuffer>& inputBuffers, VkBuffer outputBuffer);

private:
    VkDevice device;
    VkDescriptorPool descriptorPool;
    VkDescriptorSetLayout descriptorSetLayout;
};

#endif // DESCRIPTORSETMANAGER_H

// src\OnnxModelParser.h
#ifndef ONNX_MODEL_PARSER_H
#define ONNX_MODEL_PARSER_H

#include <string>
#include <vector>
#include <onnx/onnx.pb.h>

class OnnxModelParser {
private:
    onnx::ModelProto modelProto;

public:
    OnnxModelParser(const std::string& modelPath);

    const onnx::GraphProto& getGraph() const;
    std::vector<onnx::NodeProto> getNodes() const;
    onnx::TensorProto getInitializer(const std::string& name) const;
};

#endif // ONNX_MODEL_PARSER_H
// src\PipelineManager.h
#ifndef PIPELINE_MANAGER_H
#define PIPELINE_MANAGER_H

#include <vulkan/vulkan.h>
#include <unordered_map>
#include <vector>
#include <string>
#include <memory>
#include "VulkanError.h"
#include "ShaderManager.h"
#include "PushConstants.h"

struct PipelineKey {
    VulkanOperationType opType;
    std::vector<std::string> inputFormats;
    std::vector<std::string> outputFormats;
    uint32_t workgroupSizeX;
    uint32_t workgroupSizeY;
    uint32_t workgroupSizeZ;

    bool operator==(const PipelineKey& other) const {
        return opType == other.opType &&
               inputFormats == other.inputFormats &&
               outputFormats == other.outputFormats &&
               workgroupSizeX == other.workgroupSizeX &&
               workgroupSizeY == other.workgroupSizeY &&
               workgroupSizeZ == other.workgroupSizeZ;
    }
};

struct PipelineKeyHash {
    std::size_t operator()(const PipelineKey& key) const;
};

class PipelineManager {
public:
    PipelineManager(std::shared_ptr<ShaderManager> shaderManager, VkDevice device);
    ~PipelineManager();

    VkPipeline getPipeline(const PipelineKey& key);
    VkPipelineLayout getPipelineLayout(const PipelineKey& key);

private:
    std::shared_ptr<ShaderManager> shaderManager;
    VkDevice device;
    std::unordered_map<PipelineKey, VkPipeline, PipelineKeyHash> pipelines;
    std::unordered_map<PipelineKey, VkPipelineLayout, PipelineKeyHash> pipelineLayouts;

    std::string getShaderName(VulkanOperationType opType) const;
    std::vector<char> loadShaderCode(const std::string& shaderName) const;
};

#endif // PIPELINE_MANAGER_H

// src\PushConstants.h
#ifndef PUSH_CONSTANTS_H
#define PUSH_CONSTANTS_H

#include <cstdint>

struct MatMulPushConstants {
    uint32_t M;
    uint32_t K;
    uint32_t N;
};

struct Conv2DPushConstants {
    uint32_t input_width;
    uint32_t input_height;
    uint32_t input_channels;
    uint32_t output_channels;
    uint32_t kernel_size;
    uint32_t padding;
    uint32_t stride;
};

struct SoftmaxPushConstants {
    uint32_t size;
};

struct MaxPoolPushConstants {
    uint32_t width;
    uint32_t height;
    uint32_t channels;
    uint32_t batch_size;
    uint32_t poolSizeX;
    uint32_t poolSizeY;
    uint32_t strideX;
    uint32_t strideY;
};

struct BatchNormPushConstants {
    uint32_t size;
    float epsilon;
};

struct AddPushConstants {
    uint32_t total_elements;
};

struct ReLUPushConstants {
    uint32_t size;
};

struct SigmoidPushConstants {
    uint32_t size;
};

#endif // PUSH_CONSTANTS_H
// src\ShaderManager.h
#ifndef SHADER_MANAGER_H
#define SHADER_MANAGER_H

#include <vulkan/vulkan.h>
#include <string>
#include <unordered_map>
#include <memory>
#include <vector>
#include "VulkanError.h"

class ShaderManager {
public:
    ShaderManager(VkDevice device);
    ~ShaderManager();

    VkShaderModule getShaderModule(const std::string& shaderName, const std::vector<char>& code);

private:
    VkDevice device;
    std::unordered_map<std::string, VkShaderModule> shaderModules;
};

#endif // SHADER_MANAGER_H
// src\Utils.h
#ifndef UTILS_H
#define UTILS_H

#include <vulkan/vulkan.h>
#include <stdexcept>
#include <limits>
#include "VulkanError.h"

#define VK_CHECK_DETAILED(result, opType) \
    if ((result) != VK_SUCCESS) { \
        throw VulkanError("Vulkan operation failed with error code " + std::to_string(result) + \
                         " during operation " + std::to_string(static_cast<int>(opType))); \
    }

inline VkDeviceSize checkSize(size_t size) {
    if (size > static_cast<size_t>(std::numeric_limits<VkDeviceSize>::max())) {
        throw std::runtime_error("Size exceeds VkDeviceSize limit");
    }
    return static_cast<VkDeviceSize>(size);
}

#endif // UTILS_H
// src\VulkanBufferPool.h
#ifndef VULKAN_BUFFER_POOL_H
#define VULKAN_BUFFER_POOL_H

#include <vulkan/vulkan.h>
#include <queue>
#include "VulkanError.h"

class VulkanBufferPool {
public:
    VulkanBufferPool(VkDevice device);
    ~VulkanBufferPool();

    VkBuffer acquireBuffer();
    void releaseBuffer(VkBuffer buffer);

private:
    VkDevice device;
    std::queue<VkBuffer> buffers;
};

#endif // VULKAN_BUFFER_POOL_H
// src\VulkanContext.h
#ifndef VULKAN_CONTEXT_H
#define VULKAN_CONTEXT_H

#include <vulkan/vulkan.h>
#include <memory>
#include "DescriptorSetManager.h"
#include "CommandBufferManager.h"
#include "PipelineManager.h"
#include "VulkanMemoryManager.h"
#include "VulkanBufferPool.h"

class VulkanContext {
public:
    VulkanContext();
    ~VulkanContext();

    void initVulkan();
    void cleanupVulkan();

    // Accessors
    VulkanMemoryManager* getMemoryManager() const { return memoryManager.get(); }
    VulkanBufferPool* getBufferPool() const { return bufferPool.get(); }
    PipelineManager* getPipelineManager() const { return pipelineManager.get(); }
    CommandBufferManager* getCommandBufferManager() const { return commandBufferManager.get(); }
    std::shared_ptr<DescriptorSetManager> getDescriptorSetManager() const { return descriptorSetManager; }
    
    // Device accessors
    VkDevice getDevice() const { return device; }
    VkPhysicalDevice getPhysicalDevice() const { return physicalDevice; }

private:
    VkInstance instance;
    VkPhysicalDevice physicalDevice;
    VkDevice device;
    VkQueue computeQueue;
    uint32_t computeQueueFamilyIndex;

    std::unique_ptr<VulkanMemoryManager> memoryManager;
    std::unique_ptr<VulkanBufferPool> bufferPool;
    std::unique_ptr<PipelineManager> pipelineManager;
    std::unique_ptr<CommandBufferManager> commandBufferManager;
    std::shared_ptr<DescriptorSetManager> descriptorSetManager;

    void pickPhysicalDevice();
    void createLogicalDevice();
    void findComputeQueueFamily();
};

#endif // VULKAN_CONTEXT_H
// src\VulkanDeviceHelper.h
// VulkanDeviceHelper.h
#ifndef VULKAN_DEVICE_HELPER_H
#define VULKAN_DEVICE_HELPER_H

#include <vulkan/vulkan.h>
#include <vector>

class VulkanDeviceHelper {
public:
    static VkPhysicalDeviceProperties getPhysicalDeviceProperties(VkPhysicalDevice device) {
        VkPhysicalDeviceProperties properties;
        vkGetPhysicalDeviceProperties(device, &properties);
        return properties;
    }

    static uint32_t findComputeQueueFamily(VkPhysicalDevice device) {
        uint32_t queueFamilyCount = 0;
        vkGetPhysicalDeviceQueueFamilyProperties(device, &queueFamilyCount, nullptr);

        std::vector<VkQueueFamilyProperties> queueFamilies(queueFamilyCount);
        vkGetPhysicalDeviceQueueFamilyProperties(device, &queueFamilyCount, queueFamilies.data());

        for (uint32_t i = 0; i < queueFamilies.size(); i++) {
            if (queueFamilies[i].queueFlags & VK_QUEUE_COMPUTE_BIT) {
                return i;
            }
        }

        throw std::runtime_error("Could not find a compute queue family.");
    }
};

#endif // VULKAN_DEVICE_HELPER_H
// src\VulkanError.h
#ifndef VULKAN_ERROR_H
#define VULKAN_ERROR_H

#include <stdexcept>
#include <string>

// Define VulkanOperationType enum
enum class VulkanOperationType {
    MatMul,
    Conv2D,
    ReLU,
    Sigmoid,
    Softmax,
    MaxPool,
    BatchNorm,
    Add
};

class VulkanError : public std::runtime_error {
public:
    explicit VulkanError(const std::string& message) : std::runtime_error(message) {}
};

#endif // VULKAN_ERROR_H
// src\VulkanMemoryManager.h
#ifndef VULKAN_MEMORY_MANAGER_H
#define VULKAN_MEMORY_MANAGER_H

#include <vulkan/vulkan.h>
#include <vector>
#include "VulkanError.h"

class VulkanMemoryManager {
public:
    struct AllocationInfo {
        VkDeviceMemory memory;
        VkDeviceSize offset;
        VkDeviceSize size;
    };

    VulkanMemoryManager(VkPhysicalDevice physicalDevice, VkDevice device);
    ~VulkanMemoryManager();

    AllocationInfo allocate(VkDeviceSize size, VkMemoryPropertyFlags properties);

private:
    VkPhysicalDevice physicalDevice;
    VkDevice device;
    std::vector<AllocationInfo> allocations;

    uint32_t findMemoryType(uint32_t typeFilter, VkMemoryPropertyFlags properties);
};

#endif // VULKAN_MEMORY_MANAGER_H
// src\VulkanOperations.h
// src\VulkanOperations.h
#ifndef VULKAN_OPERATIONS_H
#define VULKAN_OPERATIONS_H

#include "VulkanTensor.h"
#include "PushConstants.h"

// Forward declarations
class VulkanTensor;

namespace vulkan_ops {
    // Execution functions declarations
    void executeAdd(const VulkanTensor& inputA, const VulkanTensor& inputB, VulkanTensor& output);
    
    void executeMatMul(const VulkanTensor& a, const VulkanTensor& b, VulkanTensor& c, 
                       uint32_t M, uint32_t K, uint32_t N);
    
    void executeReLU(const VulkanTensor& input, VulkanTensor& output);
    
    void executeSigmoid(const VulkanTensor& input, VulkanTensor& output);
    
    void executeSoftmax(const VulkanTensor& input, VulkanTensor& output);
    
    void executeConv2D(const VulkanTensor& input, const VulkanTensor& kernel, 
                       VulkanTensor& output, const Conv2DPushConstants& pushConstants);
    
    void executeMaxPool(const VulkanTensor& input, VulkanTensor& output, 
                        uint32_t width, uint32_t height, uint32_t channels,
                        uint32_t poolSizeX, uint32_t poolSizeY, 
                        uint32_t strideX, uint32_t strideY);
    
    void executeBatchNorm(const VulkanTensor& input, const VulkanTensor& gamma, 
                          const VulkanTensor& beta, VulkanTensor& output, 
                          uint32_t size, float epsilon);

    // Generic shader execution template
    template<typename PushConstants>
    void executeShader(VulkanOperationType opType,
                       const VulkanTensor& inputA,
                       const VulkanTensor* inputB,
                       const VulkanTensor* inputC,
                       VulkanTensor& output,
                       const PushConstants* pushConstants);
}

#endif // VULKAN_OPERATIONS_H

// src\VulkanSync.h
// VulkanSync.h
#ifndef VULKAN_SYNC_H
#define VULKAN_SYNC_H

#include <vulkan/vulkan.h>
#include "VulkanError.h"

namespace VulkanSync {
    // Wrapper class for managing Vulkan memory barriers 
    class MemoryBarrier {
    public:
        static VkMemoryBarrier getBarrier(VkAccessFlags srcAccess, VkAccessFlags dstAccess) {
            VkMemoryBarrier barrier = {};
            barrier.sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER;
            barrier.srcAccessMask = srcAccess;
            barrier.dstAccessMask = dstAccess;
            return barrier;
        }

        static VkBufferMemoryBarrier getBufferBarrier(
            VkBuffer buffer,
            VkAccessFlags srcAccess,
            VkAccessFlags dstAccess,
            VkDeviceSize offset,
            VkDeviceSize size
        ) {
            VkBufferMemoryBarrier barrier = {};
            barrier.sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER;
            barrier.srcAccessMask = srcAccess;
            barrier.dstAccessMask = dstAccess;
            barrier.buffer = buffer;
            barrier.offset = offset;
            barrier.size = size;
            barrier.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
            barrier.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
            return barrier;
        }

        static void cmdPipelineBarrier(
            VkCommandBuffer cmdBuffer,
            VkPipelineStageFlags srcStage,
            VkPipelineStageFlags dstStage,
            const VkBufferMemoryBarrier& bufferBarrier
        ) {
            vkCmdPipelineBarrier(
                cmdBuffer,
                srcStage,
                dstStage,
                0,
                0, nullptr,
                1, &bufferBarrier,
                0, nullptr
            );
        }
    };

    // RAII wrapper for GPU fence synchronization
    class ScopedGPUWait {
    private:
        VkFence fence;
        VkDevice deviceRef;

    public:
        ScopedGPUWait(VkDevice device) : deviceRef(device), fence(VK_NULL_HANDLE) {
            VkFenceCreateInfo fenceInfo = {};
            fenceInfo.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
            
            VkResult result = vkCreateFence(deviceRef, &fenceInfo, nullptr, &fence);
            if (result != VK_SUCCESS) {
                throw VulkanError("Failed to create fence during ScopedGPUWait.");
            }
        }

        ~ScopedGPUWait() {
            if (fence != VK_NULL_HANDLE) {
                vkDestroyFence(deviceRef, fence, nullptr);
                fence = VK_NULL_HANDLE;
            }
        }

        void wait() const {
            if (fence == VK_NULL_HANDLE) {
                throw VulkanError("Attempting to wait on null fence.");
            }

            VkResult result = vkWaitForFences(deviceRef, 1, &fence, VK_TRUE, UINT64_MAX);
            if (result != VK_SUCCESS) {
                throw VulkanError("Failed to wait for fence in ScopedGPUWait.");
            }

            result = vkResetFences(deviceRef, 1, &fence);
            if (result != VK_SUCCESS) {
                throw VulkanError("Failed to reset fence in ScopedGPUWait.");
            }
        }

        VkFence get() const { return fence; }

        // Delete copy operations
        ScopedGPUWait(const ScopedGPUWait&) = delete;
        ScopedGPUWait& operator=(const ScopedGPUWait&) = delete;
    };

    // Memory barrier utilities
    namespace Barriers {
        inline void recordUploadBarriers(VkCommandBuffer cmdBuffer, VkBuffer buffer, VkDeviceSize size) {
            auto barrier = MemoryBarrier::getBufferBarrier(
                buffer,
                VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT,
                VK_ACCESS_HOST_WRITE_BIT,
                0,
                size
            );

            MemoryBarrier::cmdPipelineBarrier(
                cmdBuffer,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_HOST_BIT,
                barrier
            );
        }

        inline void recordDownloadBarriers(VkCommandBuffer cmdBuffer, VkBuffer buffer, VkDeviceSize size) {
            auto barrier = MemoryBarrier::getBufferBarrier(
                buffer,
                VK_ACCESS_SHADER_WRITE_BIT,
                VK_ACCESS_HOST_READ_BIT,
                0,
                size
            );

            MemoryBarrier::cmdPipelineBarrier(
                cmdBuffer,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_HOST_BIT,
                barrier
            );
        }

        inline void recordExecutionBarriers(VkCommandBuffer cmdBuffer, VkBuffer buffer, VkDeviceSize size) {
            auto barrier = MemoryBarrier::getBufferBarrier(
                buffer,
                VK_ACCESS_SHADER_WRITE_BIT | VK_ACCESS_SHADER_READ_BIT,
                VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT,
                0,
                size
            );

            MemoryBarrier::cmdPipelineBarrier(
                cmdBuffer,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                barrier
            );
        }
    }
}

#endif // VULKAN_SYNC_H
// src\VulkanTensor.h
#ifndef VULKAN_TENSOR_H
#define VULKAN_TENSOR_H

#include <vulkan/vulkan.h>
#include <memory>
#include <string>
#include <stdexcept>
#include <cstring>
#include <algorithm>
#include <iostream>
#include "VulkanMemoryManager.h"
#include "PushConstants.h"
#include "Utils.h"
#include "VulkanBufferPool.h"
#include "PipelineManager.h"
#include "CommandBufferManager.h"
#include "vulkan_globals.h"
#include "VulkanSync.h"
#include "VulkanDeviceHelper.h"

namespace TensorLayout {
    enum class Layout {
        NHWC,  // Batch, Height, Width, Channels
        NCHW,  // Batch, Channels, Height, Width
        LINEAR // Flat layout
    };

    struct Dimensions {
        uint32_t n;         // Batch size
        uint32_t h;         // Height
        uint32_t w;         // Width
        uint32_t c;         // Channels
        Layout layout;      // Data layout
    };
}

class VulkanTensor {
private:
    VulkanMemoryManager::AllocationInfo allocation;
    TensorLayout::Dimensions dimensions;
    VkBuffer buffer;
    VulkanBufferPool* bufferPoolPtr;
    VkDevice deviceRef;

    void cleanup();

public:
    // Default constructor
    VulkanTensor();

    // Main constructor
    VulkanTensor(VulkanMemoryManager* memoryManager,
                 VulkanBufferPool* bufferPool,
                 VkDeviceSize size,
                 uint32_t w = 1,
                 uint32_t h = 1,
                 uint32_t c = 1,
                 uint32_t n = 1,
                 const void* data = nullptr,
                 TensorLayout::Layout layout = TensorLayout::Layout::LINEAR);

    // Move constructor
    VulkanTensor(VulkanTensor&& other) noexcept;

    // Move assignment operator
    VulkanTensor& operator=(VulkanTensor&& other) noexcept;

    // Destructor
    ~VulkanTensor();

    // Delete copy constructor and assignment
    VulkanTensor(const VulkanTensor&) = delete;
    VulkanTensor& operator=(const VulkanTensor&) = delete;

    // Data transfer methods
    void upload(const void* data);
    void download(void* data) const;

    // Getters
    uint32_t getN() const;
    uint32_t getC() const;
    uint32_t getH() const;
    uint32_t getW() const;
    uint32_t getWidth() const;
    uint32_t getHeight() const;
    uint32_t getChannels() const;
    VkDeviceSize getSize() const;
    VkBuffer getBuffer() const;

    // Layout methods
    TensorLayout::Layout getLayout() const;
    void setLayout(TensorLayout::Layout newLayout);

    // Utility methods
    bool isValid() const;
    std::string getDimensionsString() const;
    bool verifyDimensions(VkDeviceSize expectedSize) const;
    void debugPrint() const;
};

#endif // VULKAN_TENSOR_H
// src\vulkan_globals.h
#ifndef VULKAN_GLOBALS_H
#define VULKAN_GLOBALS_H

#include <vulkan/vulkan.h>
#include <memory>
#include <string>
#include <filesystem>
#include "VulkanContext.h"

// Namespace for global Vulkan objects
namespace vulkan_globals {
    extern VkInstance instance;
    extern VkPhysicalDevice physicalDevice;
    extern VkDevice device;
    extern VkQueue computeQueue;
    extern VkQueue graphicsQueue;
    extern VkCommandPool commandPool;
    extern VkDescriptorPool descriptorPool;

    // Add shader path handling
    extern std::filesystem::path shader_directory;
    void setShaderDirectory(const std::filesystem::path& exe_path);

    // Vulkan context instance
    extern std::unique_ptr<VulkanContext> vulkanContextInstance;

    // Functions to manage Vulkan initialization and cleanup
    bool initializeVulkan();
    void cleanupVulkan();
    VulkanContext* getContext();
}

#endif // VULKAN_GLOBALS_H
