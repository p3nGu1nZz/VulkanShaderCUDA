// src\CommandBufferManager.h
#ifndef COMMANDBUFFERMANAGER_H
#define COMMANDBUFFERMANAGER_H

#include <vulkan/vulkan.h>
#include <mutex>
#include <queue>

class CommandBufferManager {
public:
    CommandBufferManager(VkDevice device, VkCommandPool commandPool);
    ~CommandBufferManager();

    VkCommandBuffer acquireCommandBuffer();
    void releaseCommandBuffer(VkCommandBuffer commandBuffer);

private:
    VkDevice device;
    VkCommandPool commandPool;
    std::mutex mutex;
    std::queue<VkCommandBuffer> commandBuffers;
};

#endif // COMMANDBUFFERMANAGER_H

// src\DescriptorSetManager.h
#ifndef DESCRIPTORSETMANAGER_H
#define DESCRIPTORSETMANAGER_H

#include <vulkan/vulkan.h>
#include <vector>

class DescriptorSetManager {
public:
    DescriptorSetManager(VkDevice device, VkDescriptorPool descriptorPool, VkDescriptorSetLayout descriptorSetLayout);
    ~DescriptorSetManager();

    VkDescriptorSet allocateDescriptorSet();
    void updateDescriptorSet(VkDescriptorSet descriptorSet, const std::vector<VkBuffer>& inputBuffers, VkBuffer outputBuffer);

private:
    VkDevice device;
    VkDescriptorPool descriptorPool;
    VkDescriptorSetLayout descriptorSetLayout;
};

#endif // DESCRIPTORSETMANAGER_H

// src\OnnxModelParser.h
#ifndef ONNX_MODEL_PARSER_H
#define ONNX_MODEL_PARSER_H

#include <string>
#include <vector>
#include <onnx/onnx.pb.h>

class OnnxModelParser {
private:
    onnx::ModelProto modelProto;

public:
    OnnxModelParser(const std::string& modelPath);

    const onnx::GraphProto& getGraph() const;
    std::vector<onnx::NodeProto> getNodes() const;
    onnx::TensorProto getInitializer(const std::string& name) const;
};

#endif // ONNX_MODEL_PARSER_H
// src\PipelineManager.h
#ifndef PIPELINE_MANAGER_H
#define PIPELINE_MANAGER_H

#include <vulkan/vulkan.h>
#include <unordered_map>
#include <vector>
#include <string>
#include <memory>
#include "VulkanError.h"
#include "ShaderManager.h"
#include "PushConstants.h"

struct PipelineKey {
    VulkanOperationType opType;
    std::vector<std::string> inputFormats;
    std::vector<std::string> outputFormats;
    uint32_t workgroupSizeX;
    uint32_t workgroupSizeY;
    uint32_t workgroupSizeZ;

    bool operator==(const PipelineKey& other) const {
        return opType == other.opType &&
               inputFormats == other.inputFormats &&
               outputFormats == other.outputFormats &&
               workgroupSizeX == other.workgroupSizeX &&
               workgroupSizeY == other.workgroupSizeY &&
               workgroupSizeZ == other.workgroupSizeZ;
    }
};

struct PipelineKeyHash {
    std::size_t operator()(const PipelineKey& key) const;
};

class PipelineManager {
public:
    PipelineManager(std::shared_ptr<ShaderManager> shaderManager, VkDevice device);
    ~PipelineManager();

    VkPipeline getPipeline(const PipelineKey& key);
    VkPipelineLayout getPipelineLayout(const PipelineKey& key);

private:
    std::shared_ptr<ShaderManager> shaderManager;
    VkDevice device;
    std::unordered_map<PipelineKey, VkPipeline, PipelineKeyHash> pipelines;
    std::unordered_map<PipelineKey, VkPipelineLayout, PipelineKeyHash> pipelineLayouts;

    std::string getShaderName(VulkanOperationType opType) const;
    std::vector<char> loadShaderCode(const std::string& shaderName) const;
};

#endif // PIPELINE_MANAGER_H

// src\PushConstants.h
#ifndef PUSH_CONSTANTS_H
#define PUSH_CONSTANTS_H

#include <cstdint>

struct MatMulPushConstants {
    uint32_t M;
    uint32_t K;
    uint32_t N;
};

struct Conv2DPushConstants {
    uint32_t input_width;
    uint32_t input_height;
    uint32_t input_channels;
    uint32_t output_channels;
    uint32_t kernel_size;
    uint32_t padding;
    uint32_t stride;
};

struct SoftmaxPushConstants {
    uint32_t size;
};

struct MaxPoolPushConstants {
    uint32_t width;
    uint32_t height;
    uint32_t channels;
    uint32_t batch_size;
    uint32_t poolSizeX;
    uint32_t poolSizeY;
    uint32_t strideX;
    uint32_t strideY;
};

struct BatchNormPushConstants {
    uint32_t size;
    float epsilon;
};

struct AddPushConstants {
    uint32_t total_elements;
};

struct ReLUPushConstants {
    uint32_t size;
};

struct SigmoidPushConstants {
    uint32_t size;
};

#endif // PUSH_CONSTANTS_H
// src\ShaderManager.h
#ifndef SHADER_MANAGER_H
#define SHADER_MANAGER_H

#include <vulkan/vulkan.h>
#include <string>
#include <unordered_map>
#include <memory>
#include <vector>
#include "VulkanError.h"

class ShaderManager {
public:
    ShaderManager(VkDevice device);
    ~ShaderManager();

    VkShaderModule getShaderModule(const std::string& shaderName, const std::vector<char>& code);

private:
    VkDevice device;
    std::unordered_map<std::string, VkShaderModule> shaderModules;
};

#endif // SHADER_MANAGER_H
// src\Utils.h
#ifndef UTILS_H
#define UTILS_H

#include <vulkan/vulkan.h>
#include <stdexcept>
#include <limits>
#include "VulkanError.h"

#define VK_CHECK_DETAILED(result, opType) \
    if ((result) != VK_SUCCESS) { \
        throw VulkanError("Vulkan operation failed with error code " + std::to_string(result) + \
                         " during operation " + std::to_string(static_cast<int>(opType))); \
    }

inline VkDeviceSize checkSize(size_t size) {
    if (size > static_cast<size_t>(std::numeric_limits<VkDeviceSize>::max())) {
        throw std::runtime_error("Size exceeds VkDeviceSize limit");
    }
    return static_cast<VkDeviceSize>(size);
}

#endif // UTILS_H
// src\VulkanBufferPool.h
#ifndef VULKAN_BUFFER_POOL_H
#define VULKAN_BUFFER_POOL_H

#include <vulkan/vulkan.h>
#include <queue>
#include "VulkanError.h"

class VulkanBufferPool {
public:
    VulkanBufferPool(VkDevice device);
    ~VulkanBufferPool();

    VkBuffer acquireBuffer();
    void releaseBuffer(VkBuffer buffer);

private:
    VkDevice device;
    std::queue<VkBuffer> buffers;
};

#endif // VULKAN_BUFFER_POOL_H
// src\VulkanContext.h
#ifndef VULKAN_CONTEXT_H
#define VULKAN_CONTEXT_H

#include <vulkan/vulkan.h>
#include <memory>
#include "DescriptorSetManager.h"
#include "CommandBufferManager.h"
#include "PipelineManager.h"
#include "VulkanMemoryManager.h"
#include "VulkanBufferPool.h"

// VulkanContext: Manages Vulkan resources and operations
class VulkanContext {
public:
    VulkanContext();
    ~VulkanContext();

    void initVulkan();   // Initialize Vulkan resources
    void cleanupVulkan(); // Cleanup Vulkan resources

    // Accessors
    VulkanMemoryManager* getMemoryManager() const { return memoryManager.get(); }
    VulkanBufferPool* getBufferPool() const { return bufferPool.get(); }
    PipelineManager* getPipelineManager() const { return pipelineManager.get(); }
    CommandBufferManager* getCommandBufferManager() const { return commandBufferManager.get(); }
    std::shared_ptr<DescriptorSetManager> getDescriptorSetManager() const { return descriptorSetManager; }

private:
    // Vulkan instance and devices
    VkInstance instance;
    VkPhysicalDevice physicalDevice;
    VkDevice device;
    VkQueue computeQueue;
    uint32_t computeQueueFamilyIndex;

    // Managers
    std::unique_ptr<VulkanMemoryManager> memoryManager;
    std::unique_ptr<VulkanBufferPool> bufferPool;
    std::unique_ptr<PipelineManager> pipelineManager;
    std::unique_ptr<CommandBufferManager> commandBufferManager;
    std::shared_ptr<DescriptorSetManager> descriptorSetManager;

    // Helper functions
    void pickPhysicalDevice();
    void createLogicalDevice();
    void findComputeQueueFamily();
};

#endif // VULKAN_CONTEXT_H

// src\VulkanError.h
#ifndef VULKAN_ERROR_H
#define VULKAN_ERROR_H

#include <stdexcept>
#include <string>

// Define VulkanOperationType enum
enum class VulkanOperationType {
    MatMul,
    Conv2D,
    ReLU,
    Sigmoid,
    Softmax,
    MaxPool,
    BatchNorm,
    Add
};

class VulkanError : public std::runtime_error {
public:
    explicit VulkanError(const std::string& message) : std::runtime_error(message) {}
};

#endif // VULKAN_ERROR_H
// src\VulkanMemoryManager.h
#ifndef VULKAN_MEMORY_MANAGER_H
#define VULKAN_MEMORY_MANAGER_H

#include <vulkan/vulkan.h>
#include <vector>
#include "VulkanError.h"

class VulkanMemoryManager {
public:
    struct AllocationInfo {
        VkDeviceMemory memory;
        VkDeviceSize offset;
        VkDeviceSize size;
    };

    VulkanMemoryManager(VkPhysicalDevice physicalDevice, VkDevice device);
    ~VulkanMemoryManager();

    AllocationInfo allocate(VkDeviceSize size, VkMemoryPropertyFlags properties);

private:
    VkPhysicalDevice physicalDevice;
    VkDevice device;
    std::vector<AllocationInfo> allocations;

    uint32_t findMemoryType(uint32_t typeFilter, VkMemoryPropertyFlags properties);
};

#endif // VULKAN_MEMORY_MANAGER_H
// src\VulkanOperations.h
#ifndef VULKAN_OPERATIONS_H
#define VULKAN_OPERATIONS_H

#include "VulkanTensor.h"
#include "PushConstants.h"

// Forward declarations
class VulkanTensor;

namespace vulkan_ops {
    // Execution functions declarations
    void executeAdd(const VulkanTensor& inputA, const VulkanTensor& inputB, VulkanTensor& output);
    
    void executeMatMul(const VulkanTensor& a, const VulkanTensor& b, VulkanTensor& c, 
                      uint32_t M, uint32_t K, uint32_t N);
    
    void executeReLU(const VulkanTensor& input, VulkanTensor& output);
    
    void executeSigmoid(const VulkanTensor& input, VulkanTensor& output);
    
    void executeSoftmax(const VulkanTensor& input, VulkanTensor& output);
    
    void executeConv2D(const VulkanTensor& input, const VulkanTensor& kernel, 
                      VulkanTensor& output, const Conv2DPushConstants& pushConstants);
    
    void executeMaxPool(const VulkanTensor& input, VulkanTensor& output, 
                       uint32_t width, uint32_t height, uint32_t channels,
                       uint32_t poolSizeX, uint32_t poolSizeY, 
                       uint32_t strideX, uint32_t strideY);
    
    void executeBatchNorm(const VulkanTensor& input, const VulkanTensor& gamma, 
                         const VulkanTensor& beta, VulkanTensor& output, 
                         uint32_t size, float epsilon);

    // Generic shader execution template
    template<typename PushConstants>
    void executeShader(VulkanOperationType opType,
                      const VulkanTensor& inputA,
                      const VulkanTensor* inputB,
                      const VulkanTensor* inputC,
                      VulkanTensor& output,
                      const PushConstants* pushConstants);
}

#endif // VULKAN_OPERATIONS_H
// src\VulkanSync.h
#ifndef VULKAN_SYNC_H
#define VULKAN_SYNC_H

#include <vulkan/vulkan.h>
#include "VulkanError.h"

class ScopedGPUWait {
private:
    VkFence fence;
    VkDevice deviceRef;

public:
    ScopedGPUWait(VkDevice device);
    ~ScopedGPUWait();

    VkFence get() const;
    void wait() const;

    // Delete copy constructors
    ScopedGPUWait(const ScopedGPUWait&) = delete;
    ScopedGPUWait& operator=(const ScopedGPUWait&) = delete;
};

#endif // VULKAN_SYNC_H
// src\VulkanTensor.h
#ifndef VULKAN_TENSOR_H
#define VULKAN_TENSOR_H

#include <vulkan/vulkan.h>
#include <memory>
#include <string>
#include <stdexcept>
#include <cstring>
#include "VulkanMemoryManager.h"
#include "PushConstants.h"
#include "Utils.h"
#include "VulkanBufferPool.h"
#include "PipelineManager.h"
#include "CommandBufferManager.h"
#include "vulkan_globals.h"

// Tensor Layout Namespace
namespace TensorLayout {
    enum class Layout {
        NHWC,  // Batch, Height, Width, Channels
        NCHW,  // Batch, Channels, Height, Width
        LINEAR // Flat layout
    };

    struct Dimensions {
        uint32_t n;         // Batch size
        uint32_t h;         // Height
        uint32_t w;         // Width
        uint32_t c;         // Channels
        Layout layout;      // Data layout
    };
}

// Vulkan Sync Namespace
namespace VulkanSync {
    class ScopedGPUWait {
    private:
        VkFence fence;
        VkDevice deviceRef;

    public:
        ScopedGPUWait(VkDevice device) : deviceRef(device) {
            VkFenceCreateInfo fenceInfo = {};
            fenceInfo.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
            fenceInfo.flags = 0;

            VkResult result = vkCreateFence(deviceRef, &fenceInfo, nullptr, &fence);
            if (result != VK_SUCCESS) {
                throw VulkanError("Failed to create fence during ScopedGPUWait.");
            }
        }

        ~ScopedGPUWait() {
            if (fence != VK_NULL_HANDLE) {
                vkDestroyFence(deviceRef, fence, nullptr);
            }
        }

        VkFence get() const { return fence; }

        void wait() const {
            VkResult result = vkWaitForFences(deviceRef, 1, &fence, VK_TRUE, UINT64_MAX);
            if (result != VK_SUCCESS) {
                throw VulkanError("Failed to wait for fence in ScopedGPUWait.");
            }
        }

        // Delete copy constructors
        ScopedGPUWait(const ScopedGPUWait&) = delete;
        ScopedGPUWait& operator=(const ScopedGPUWait&) = delete;
    };
};

class VulkanTensor {
private:
    VulkanMemoryManager::AllocationInfo allocation; // Memory allocation info
    TensorLayout::Dimensions dimensions;            // Tensor dimensions
    VkBuffer buffer;                                // Vulkan buffer handle
    VulkanBufferPool* bufferPoolPtr;                // Pointer to the buffer pool
    VkDevice deviceRef;                             // Vulkan device reference

public:
    VulkanTensor(VulkanMemoryManager* memoryManager, 
                 VulkanBufferPool* bufferPool, 
                 VkDeviceSize size,
                 uint32_t w = 1, 
                 uint32_t h = 1, 
                 uint32_t c = 1, 
                 uint32_t n = 1,
                 const void* data = nullptr, 
                 TensorLayout::Layout layout = TensorLayout::Layout::LINEAR)
        : dimensions{n, h, w, c, layout}
        , bufferPoolPtr(bufferPool)
        , deviceRef(vulkan_globals::device) {
        
        if (deviceRef == VK_NULL_HANDLE) {
            throw VulkanError("Vulkan device not initialized.");
        }

        // Allocate memory
        allocation = memoryManager->allocate(
            size, 
            VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT
        );
        
        // Create buffer
        VkBufferCreateInfo bufferInfo = {};
        bufferInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
        bufferInfo.size = size;
        bufferInfo.usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | 
                           VK_BUFFER_USAGE_TRANSFER_DST_BIT | 
                           VK_BUFFER_USAGE_TRANSFER_SRC_BIT;
        bufferInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;

        VkResult result = vkCreateBuffer(deviceRef, &bufferInfo, nullptr, &buffer);
        if (result != VK_SUCCESS) {
            throw VulkanError("Failed to create buffer in VulkanTensor.");
        }

        // Bind buffer memory
        result = vkBindBufferMemory(deviceRef, buffer, allocation.memory, allocation.offset);
        if (result != VK_SUCCESS) {
            vkDestroyBuffer(deviceRef, buffer, nullptr);
            throw VulkanError("Failed to bind buffer memory in VulkanTensor.");
        }

        // Upload data if provided
        if (data) {
            upload(data);
        }
    }

    // Move constructor
    VulkanTensor(VulkanTensor&& other) noexcept
        : allocation(other.allocation)
        , dimensions(other.dimensions)
        , buffer(other.buffer)
        , bufferPoolPtr(other.bufferPoolPtr)
        , deviceRef(other.deviceRef) {
        other.buffer = VK_NULL_HANDLE;
        other.bufferPoolPtr = nullptr;
    }

    // Move assignment operator
    VulkanTensor& operator=(VulkanTensor&& other) noexcept {
        if (this != &other) {
            cleanup();

            allocation = other.allocation;
            dimensions = other.dimensions;
            buffer = other.buffer;
            bufferPoolPtr = other.bufferPoolPtr;
            deviceRef = other.deviceRef;

            other.buffer = VK_NULL_HANDLE;
            other.bufferPoolPtr = nullptr;
        }
        return *this;
    }

    // Destructor
    ~VulkanTensor() {
        cleanup();
    }

    // Delete copy constructor and assignment
    VulkanTensor(const VulkanTensor&) = delete;
    VulkanTensor& operator=(const VulkanTensor&) = delete;

    // Upload data to Vulkan buffer
    void upload(const void* data) {
        if (!data) {
            throw std::runtime_error("Null pointer provided for upload.");
        }

        void* mappedMemory;
        VkResult result = vkMapMemory(deviceRef, allocation.memory, allocation.offset, allocation.size, 0, &mappedMemory);
        if (result != VK_SUCCESS) {
            throw VulkanError("Failed to map memory for upload in VulkanTensor.");
        }

        memcpy(mappedMemory, data, static_cast<size_t>(allocation.size));

        VkMappedMemoryRange memoryRange = {};
        memoryRange.sType = VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE;
        memoryRange.memory = allocation.memory;
        memoryRange.offset = allocation.offset;
        memoryRange.size = allocation.size;
        vkFlushMappedMemoryRanges(deviceRef, 1, &memoryRange);

        vkUnmapMemory(deviceRef, allocation.memory);
    }

    // Download data from Vulkan buffer
    void download(void* data) const {
        if (!data) {
            throw std::runtime_error("Null pointer provided for download.");
        }

        void* mappedMemory;
        VkResult result = vkMapMemory(deviceRef, allocation.memory, allocation.offset, allocation.size, 0, &mappedMemory);
        if (result != VK_SUCCESS) {
            throw VulkanError("Failed to map memory for download in VulkanTensor.");
        }

        VkMappedMemoryRange memoryRange = {};
        memoryRange.sType = VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE;
        memoryRange.memory = allocation.memory;
        memoryRange.offset = allocation.offset;
        memoryRange.size = allocation.size;
        vkInvalidateMappedMemoryRanges(deviceRef, 1, &memoryRange);

        memcpy(data, mappedMemory, static_cast<size_t>(allocation.size));
        vkUnmapMemory(deviceRef, allocation.memory);
    }

    // Convert tensor layout
    void convertLayout(TensorLayout::Layout newLayout, CommandBufferManager& cmdManager, PipelineManager& pipelineManager);

    // Getters
    VkDeviceSize getSize() const { return allocation.size; }
    uint32_t getWidth() const { return dimensions.w; }
    uint32_t getHeight() const { return dimensions.h; }
    uint32_t getChannels() const { return dimensions.c; }
    uint32_t getDepth() const { return dimensions.n; }
    VkBuffer getBuffer() const { return buffer; }

    // Utility methods
    bool isValid() const {
        return buffer != VK_NULL_HANDLE && allocation.memory != VK_NULL_HANDLE;
    }

    std::string getDimensionsString() const {
        return "(" + std::to_string(dimensions.n) + ", " +
               std::to_string(dimensions.c) + ", " +
               std::to_string(dimensions.h) + ", " +
               std::to_string(dimensions.w) + ")";
    }

    bool verifyDimensions(VkDeviceSize expectedSize) const {
        return allocation.size == expectedSize &&
               (dimensions.w * dimensions.h * dimensions.c * dimensions.n * sizeof(float)) == expectedSize;
    }

private:
    void cleanup() {
        if (buffer != VK_NULL_HANDLE) {
            if (bufferPoolPtr) {
                bufferPoolPtr->releaseBuffer(buffer);
            }
            vkDestroyBuffer(deviceRef, buffer, nullptr);
            buffer = VK_NULL_HANDLE;
        }
    }
};

#endif // VULKAN_TENSOR_H

// src\vulkan_globals.h
#ifndef VULKAN_GLOBALS_H
#define VULKAN_GLOBALS_H

#include <vulkan/vulkan.h>
#include <memory>
#include "VulkanContext.h"

// Namespace for global Vulkan objects
namespace vulkan_globals {
    extern VkInstance instance;
    extern VkPhysicalDevice physicalDevice;
    extern VkDevice device;
    extern VkQueue computeQueue;
    extern VkQueue graphicsQueue;
    extern VkCommandPool commandPool;
    extern VkDescriptorPool descriptorPool;

    // Vulkan context instance
    extern std::unique_ptr<VulkanContext> vulkanContextInstance;

    // Functions to manage Vulkan initialization and cleanup
    bool initializeVulkan();
    void cleanupVulkan();
    VulkanContext* getContext();
}

#endif // VULKAN_GLOBALS_H

