// src\CommandBufferManager.cpp
// src\CommandBufferManager.cpp
#include "CommandBufferManager.h"
#include "spdlog/spdlog.h"

// Constructor
CommandBufferManager::CommandBufferManager(VkDevice device, VkCommandPool commandPool)
    : device(device), commandPool(commandPool) {
    spdlog::info("CommandBufferManager created.");
}

// Destructor: Cleans up all command buffers in the pool
CommandBufferManager::~CommandBufferManager() {
    std::lock_guard<std::mutex> lock(mutex);

    while (!commandBuffers.empty()) {
        VkCommandBuffer cmd = commandBuffers.front();
        vkFreeCommandBuffers(device, commandPool, 1, &cmd);
        commandBuffers.pop();
    }
    spdlog::info("CommandBufferManager destroyed.");
}

// Acquire a command buffer from the pool or allocate a new one if the pool is empty
VkCommandBuffer CommandBufferManager::acquireCommandBuffer() {
    std::lock_guard<std::mutex> lock(mutex);

    if (!commandBuffers.empty()) {
        VkCommandBuffer cmd = commandBuffers.front();
        commandBuffers.pop();
        spdlog::debug("Acquired command buffer from pool.");
        return cmd;
    }

    VkCommandBufferAllocateInfo allocInfo = {};
    allocInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO;
    allocInfo.commandPool = commandPool;
    allocInfo.level = VK_COMMAND_BUFFER_LEVEL_PRIMARY;
    allocInfo.commandBufferCount = 1;

    VkCommandBuffer cmdBuffer;
    if (vkAllocateCommandBuffers(device, &allocInfo, &cmdBuffer) != VK_SUCCESS) {
        throw std::runtime_error("Failed to allocate command buffer.");
    }

    spdlog::debug("Allocated new command buffer.");
    return cmdBuffer;
}

// Release a command buffer back to the pool for reuse
void CommandBufferManager::releaseCommandBuffer(VkCommandBuffer cmd) {
    std::lock_guard<std::mutex> lock(mutex);
    commandBuffers.push(cmd);
    spdlog::debug("Released command buffer back to pool.");
}

// src\DescriptorSetManager.cpp
// src\DescriptorSetManager.cpp
#include "DescriptorSetManager.h"
#include "spdlog/spdlog.h"

DescriptorSetManager::DescriptorSetManager(VkDevice device, VkDescriptorPool descriptorPool, VkDescriptorSetLayout descriptorSetLayout)
    : device(device), descriptorPool(descriptorPool), descriptorSetLayout(descriptorSetLayout) {
    spdlog::info("DescriptorSetManager created.");
}

DescriptorSetManager::~DescriptorSetManager() {
    spdlog::info("DescriptorSetManager destroyed.");
}

VkDescriptorSet DescriptorSetManager::allocateDescriptorSet() {
    VkDescriptorSetAllocateInfo allocInfo = {};
    allocInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO;
    allocInfo.descriptorPool = descriptorPool;
    allocInfo.descriptorSetCount = 1;
    allocInfo.pSetLayouts = &descriptorSetLayout;

    VkDescriptorSet descriptorSet;
    if (vkAllocateDescriptorSets(device, &allocInfo, &descriptorSet) != VK_SUCCESS) {
        throw std::runtime_error("Failed to allocate descriptor set.");
    }

    return descriptorSet;
}

void DescriptorSetManager::updateDescriptorSet(VkDescriptorSet descriptorSet, const std::vector<VkBuffer>& inputBuffers, VkBuffer outputBuffer) {
    std::vector<VkDescriptorBufferInfo> bufferInfos;
    std::vector<VkWriteDescriptorSet> descriptorWrites;

    // Reserve space to avoid reallocations
    bufferInfos.reserve(inputBuffers.size() + 1);
    descriptorWrites.reserve(inputBuffers.size() + 1);

    // Process input buffers
    for (size_t i = 0; i < inputBuffers.size(); i++) {
        VkDescriptorBufferInfo bufferInfo = {};
        bufferInfo.buffer = inputBuffers[i];
        bufferInfo.offset = 0;
        bufferInfo.range = VK_WHOLE_SIZE;
        bufferInfos.push_back(bufferInfo);

        VkWriteDescriptorSet writeDescriptor = {};
        writeDescriptor.sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
        writeDescriptor.dstSet = descriptorSet;
        writeDescriptor.dstBinding = static_cast<uint32_t>(i); // Binding index for input buffers
        writeDescriptor.dstArrayElement = 0;
        writeDescriptor.descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
        writeDescriptor.descriptorCount = 1;
        writeDescriptor.pBufferInfo = &bufferInfos.back();
        descriptorWrites.push_back(writeDescriptor);
    }

    // Add output buffer
    if (outputBuffer != VK_NULL_HANDLE) {
        VkDescriptorBufferInfo outputBufferInfo = {};
        outputBufferInfo.buffer = outputBuffer;
        outputBufferInfo.offset = 0;
        outputBufferInfo.range = VK_WHOLE_SIZE;
        bufferInfos.push_back(outputBufferInfo);

        VkWriteDescriptorSet writeDescriptor = {};
        writeDescriptor.sType = VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET;
        writeDescriptor.dstSet = descriptorSet;
        writeDescriptor.dstBinding = static_cast<uint32_t>(inputBuffers.size()); // Binding index for output buffer
        writeDescriptor.dstArrayElement = 0;
        writeDescriptor.descriptorType = VK_DESCRIPTOR_TYPE_STORAGE_BUFFER;
        writeDescriptor.descriptorCount = 1;
        writeDescriptor.pBufferInfo = &bufferInfos.back();
        descriptorWrites.push_back(writeDescriptor);
    }

    // Update all descriptors at once
    vkUpdateDescriptorSets(
        device,
        static_cast<uint32_t>(descriptorWrites.size()),
        descriptorWrites.data(),
        0,
        nullptr
    );
}

// src\OnnxModelParser.cpp
#include "OnnxModelParser.h"
#include <fstream>
#include <stdexcept>
#include <google/protobuf/io/zero_copy_stream_impl.h>
#include <google/protobuf/text_format.h>

OnnxModelParser::OnnxModelParser(const std::string& modelPath) {
    std::ifstream modelFile(modelPath, std::ios::binary);
    if (!modelFile.is_open()) {
        throw std::runtime_error("Failed to open ONNX model file: " + modelPath);
    }

    google::protobuf::io::IstreamInputStream input(&modelFile);
    if (!modelProto.ParseFromIstream(&modelFile)) {
        modelFile.close();
        throw std::runtime_error("Failed to parse ONNX model.");
    }

    modelFile.close();
}

const onnx::GraphProto& OnnxModelParser::getGraph() const {
    return modelProto.graph();
}

std::vector<onnx::NodeProto> OnnxModelParser::getNodes() const {
    return std::vector<onnx::NodeProto>(modelProto.graph().node().begin(), 
                                      modelProto.graph().node().end());
}

onnx::TensorProto OnnxModelParser::getInitializer(const std::string& name) const {
    for (const auto& initializer : modelProto.graph().initializer()) {
        if (initializer.name() == name) {
            return initializer;
        }
    }
    throw std::runtime_error("Initializer not found: " + name);
}
// src\PipelineManager.cpp
// src\PipelineManager.cpp
#include "PipelineManager.h"
#include "vulkan_globals.h"
#include "spdlog/spdlog.h"
#include <fstream>
#include <filesystem>

// Hash function implementation
std::size_t PipelineKeyHash::operator()(const PipelineKey& key) const {
    std::size_t res = std::hash<int>()(static_cast<int>(key.opType));
    
    // Combine hash of input formats
    for (const auto& format : key.inputFormats) {
        res ^= std::hash<std::string>()(format) + 0x9e3779b9 + (res << 6) + (res >> 2);
    }

    // Combine hash of output formats
    for (const auto& format : key.outputFormats) {
        res ^= std::hash<std::string>()(format) + 0x9e3779b9 + (res << 6) + (res >> 2);
    }

    // Combine workgroup size hashes
    res ^= std::hash<uint32_t>()(key.workgroupSizeX) + 0x9e3779b9 + (res << 6) + (res >> 2);
    res ^= std::hash<uint32_t>()(key.workgroupSizeY) + 0x9e3779b9 + (res << 6) + (res >> 2);
    res ^= std::hash<uint32_t>()(key.workgroupSizeZ) + 0x9e3779b9 + (res << 6) + (res >> 2);

    return res;
}

PipelineManager::PipelineManager(std::shared_ptr<ShaderManager> shaderManager, VkDevice device)
    : shaderManager(shaderManager), device(device) {}

PipelineManager::~PipelineManager() {
    // Clean up pipelines
    for (auto& [key, pipeline] : pipelines) {
        vkDestroyPipeline(device, pipeline, nullptr);
        spdlog::info("Pipeline destroyed.");
    }

    // Clean up pipeline layouts
    for (auto& [key, layout] : pipelineLayouts) {
        vkDestroyPipelineLayout(device, layout, nullptr);
        spdlog::info("Pipeline layout destroyed.");
    }
}

std::string PipelineManager::getShaderName(VulkanOperationType opType) const {
    switch (opType) {
        case VulkanOperationType::MatMul:   return "matmul.comp.spv";
        case VulkanOperationType::Conv2D:   return "conv2d.comp.spv";
        case VulkanOperationType::ReLU:     return "relu.comp.spv";
        case VulkanOperationType::Sigmoid:  return "sigmoid.comp.spv";
        case VulkanOperationType::Softmax:  return "softmax.comp.spv";
        case VulkanOperationType::MaxPool:  return "pooling.comp.spv";
        case VulkanOperationType::BatchNorm: return "batchnorm.comp.spv";
        case VulkanOperationType::Add:      return "add.comp.spv";
        default:
            throw VulkanError("Unknown Vulkan operation type.");
    }
}

std::vector<char> PipelineManager::loadShaderCode(const std::string& shaderName) const {
    std::filesystem::path shaderPath = vulkan_globals::shader_directory / shaderName;
    
    // Convert path to string for opening file
    std::string shaderPathStr = shaderPath.string();
    std::ifstream file(shaderPathStr, std::ios::binary | std::ios::ate);
    
    if (!file.is_open()) {
        throw VulkanError("Failed to open shader file: " + shaderPathStr);
    }

    size_t fileSize = static_cast<size_t>(file.tellg());
    std::vector<char> buffer(fileSize);

    file.seekg(0);
    file.read(buffer.data(), fileSize);
    file.close();

    spdlog::info("Successfully loaded shader: {}", shaderPathStr);
    return buffer;
}

VkPipeline PipelineManager::getPipeline(const PipelineKey& key) {
    // Check if pipeline already exists
    auto it = pipelines.find(key);
    if (it != pipelines.end()) {
        return it->second;
    }

    // Create new pipeline layout
    VkPipelineLayout pipelineLayout = getPipelineLayout(key);

    // Load shader module
    std::string shaderName = getShaderName(key.opType);
    std::vector<char> shaderCode = loadShaderCode(shaderName);
    VkShaderModule shaderModule = shaderManager->getShaderModule(shaderName, shaderCode);

    // Create compute pipeline
    VkComputePipelineCreateInfo pipelineInfo = {};
    pipelineInfo.sType = VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO;
    pipelineInfo.layout = pipelineLayout;
    pipelineInfo.stage.sType = VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO;
    pipelineInfo.stage.stage = VK_SHADER_STAGE_COMPUTE_BIT;
    pipelineInfo.stage.module = shaderModule;
    pipelineInfo.stage.pName = "main";

    VkPipeline pipeline;
    VkResult result = vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipelineInfo, nullptr, &pipeline);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to create compute pipeline.");
    }

    // Store and return the created pipeline
    pipelines[key] = pipeline;
    spdlog::info("Compute pipeline created for operation type: {}", static_cast<int>(key.opType));
    return pipeline;
}

VkPipelineLayout PipelineManager::getPipelineLayout(const PipelineKey& key) {
    auto it = pipelineLayouts.find(key);
    if (it != pipelineLayouts.end()) {
        return it->second;
    }

    VkPushConstantRange pushConstantRange = {};
    pushConstantRange.stageFlags = VK_SHADER_STAGE_COMPUTE_BIT;

    switch (key.opType) {
        case VulkanOperationType::MatMul:
            pushConstantRange.size = sizeof(MatMulPushConstants);
            break;
        case VulkanOperationType::Conv2D:
            pushConstantRange.size = sizeof(Conv2DPushConstants);
            break;
        case VulkanOperationType::ReLU:
            pushConstantRange.size = sizeof(ReLUPushConstants);
            break;
        case VulkanOperationType::Sigmoid:
            pushConstantRange.size = sizeof(SigmoidPushConstants);
            break;
        case VulkanOperationType::Softmax:
            pushConstantRange.size = sizeof(SoftmaxPushConstants);
            break;
        case VulkanOperationType::MaxPool:
            pushConstantRange.size = sizeof(MaxPoolPushConstants);
            break;
        case VulkanOperationType::BatchNorm:
            pushConstantRange.size = sizeof(BatchNormPushConstants);
            break;
        case VulkanOperationType::Add:
            pushConstantRange.size = sizeof(AddPushConstants);
            break;
        default:
            pushConstantRange.size = 0;
    }

    // Create descriptor set layout
    std::vector<VkDescriptorSetLayoutBinding> bindings = {
        {
            0,                                  // binding
            VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,  // descriptorType
            1,                                  // descriptorCount
            VK_SHADER_STAGE_COMPUTE_BIT,       // stageFlags
            nullptr                            // pImmutableSamplers
        },
        {
            1,                                  // binding
            VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,  // descriptorType
            1,                                  // descriptorCount
            VK_SHADER_STAGE_COMPUTE_BIT,       // stageFlags
            nullptr                            // pImmutableSamplers
        },
        {
            2,                                  // binding
            VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,  // descriptorType
            1,                                  // descriptorCount
            VK_SHADER_STAGE_COMPUTE_BIT,       // stageFlags
            nullptr                            // pImmutableSamplers
        }
    };

    VkDescriptorSetLayoutCreateInfo layoutInfo = {};
    layoutInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
    layoutInfo.bindingCount = static_cast<uint32_t>(bindings.size());
    layoutInfo.pBindings = bindings.data();

    VkDescriptorSetLayout descriptorSetLayout;
    if (vkCreateDescriptorSetLayout(device, &layoutInfo, nullptr, &descriptorSetLayout) != VK_SUCCESS) {
        throw VulkanError("Failed to create descriptor set layout");
    }

    // Create pipeline layout
    VkPipelineLayoutCreateInfo pipelineLayoutInfo = {};
    pipelineLayoutInfo.sType = VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO;
    pipelineLayoutInfo.setLayoutCount = 1;
    pipelineLayoutInfo.pSetLayouts = &descriptorSetLayout;
    
    if (pushConstantRange.size > 0) {
        pipelineLayoutInfo.pushConstantRangeCount = 1;
        pipelineLayoutInfo.pPushConstantRanges = &pushConstantRange;
    }

    VkPipelineLayout pipelineLayout;
    VkResult result = vkCreatePipelineLayout(device, &pipelineLayoutInfo, nullptr, &pipelineLayout);
    if (result != VK_SUCCESS) {
        vkDestroyDescriptorSetLayout(device, descriptorSetLayout, nullptr);
        throw VulkanError("Failed to create pipeline layout.");
    }

    // Store layout in cache
    pipelineLayouts[key] = pipelineLayout;
    return pipelineLayout;
}
// src\ShaderManager.cpp
// src\ShaderManager.cpp
#include "ShaderManager.h"
#include <spdlog/spdlog.h>

ShaderManager::ShaderManager(VkDevice device)
    : device(device) {}

ShaderManager::~ShaderManager() {
    for (auto& [name, shaderModule] : shaderModules) {
        vkDestroyShaderModule(device, shaderModule, nullptr);
        spdlog::info("Shader module destroyed: {}", name);
    }
}

VkShaderModule ShaderManager::getShaderModule(const std::string& shaderName, const std::vector<char>& code) {
    auto it = shaderModules.find(shaderName);
    if (it != shaderModules.end()) {
        return it->second;
    }

    VkShaderModuleCreateInfo createInfo = {};
    createInfo.sType = VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO;
    createInfo.codeSize = code.size();

    // Ensure code is aligned to 4 bytes as required by Vulkan
    if (code.size() % 4 != 0) {
        throw VulkanError("Shader code size is not a multiple of 4 bytes.");
    }
    createInfo.pCode = reinterpret_cast<const uint32_t*>(code.data());

    VkShaderModule shaderModule;
    VkResult result = vkCreateShaderModule(device, &createInfo, nullptr, &shaderModule);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to create shader module: " + shaderName);
    }

    // Optional: Verify shader compilation by creating a dummy pipeline
    // This step is skipped here but recommended during development using validation layers

    shaderModules[shaderName] = shaderModule;
    spdlog::info("Shader module created: {}", shaderName);
    return shaderModule;
}

// src\VulkanBufferPool.cpp
// src\VulkanBufferPool.cpp

#include "VulkanBufferPool.h"

// Constructor
VulkanBufferPool::VulkanBufferPool(VkDevice device)
    : device(device) {}

// Destructor: Cleans up all buffers in the pool
VulkanBufferPool::~VulkanBufferPool() {
    while (!buffers.empty()) {
        VkBuffer buffer = buffers.front();
        buffers.pop();
        vkDestroyBuffer(device, buffer, nullptr);
    }
}

// Acquire a buffer from the pool or create a new one if the pool is empty
VkBuffer VulkanBufferPool::acquireBuffer() {
    if (!buffers.empty()) {
        VkBuffer buf = buffers.front();
        buffers.pop();
        return buf;
    }

    // Create a new buffer with predefined size and usage
    VkBufferCreateInfo bufferInfo = {};
    bufferInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
    bufferInfo.size = 1024 * 1024; // 1MB buffer, adjust as needed
    bufferInfo.usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_SRC_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT;
    bufferInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;

    VkBuffer buffer;
    VkResult result = vkCreateBuffer(device, &bufferInfo, nullptr, &buffer);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to create buffer in VulkanBufferPool.");
    }

    return buffer;
}

// Release a buffer back to the pool for reuse
void VulkanBufferPool::releaseBuffer(VkBuffer buffer) {
    buffers.push(buffer);
}

// src\VulkanContext.cpp
// src\VulkanContext.cpp
#include "VulkanContext.h"
#include "vulkan_globals.h"
#include "spdlog/spdlog.h"

#include <stdexcept>
#include <vector>
#include <algorithm>

// Constructor
VulkanContext::VulkanContext()
    : instance(VK_NULL_HANDLE), physicalDevice(VK_NULL_HANDLE),
      device(VK_NULL_HANDLE), computeQueue(VK_NULL_HANDLE),
      computeQueueFamilyIndex(std::numeric_limits<uint32_t>::max()) {}

// Destructor
VulkanContext::~VulkanContext() {
    cleanupVulkan();
}

// Initialize Vulkan resources
void VulkanContext::initVulkan() {
    spdlog::info("Creating Vulkan instance...");

    // 1. Create Vulkan Instance
    VkApplicationInfo appInfo = {};
    appInfo.sType = VK_STRUCTURE_TYPE_APPLICATION_INFO;
    appInfo.pApplicationName = "VulkanBackend";
    appInfo.applicationVersion = VK_MAKE_VERSION(1, 0, 0);
    appInfo.pEngineName = "NoEngine";
    appInfo.engineVersion = VK_MAKE_VERSION(1, 0, 0);
    appInfo.apiVersion = VK_API_VERSION_1_2; // Ensure compatibility

    VkInstanceCreateInfo createInfo = {};
    createInfo.sType = VK_STRUCTURE_TYPE_INSTANCE_CREATE_INFO;
    createInfo.pApplicationInfo = &appInfo;

    // Optional: Enable validation layers during development
    std::vector<const char*> validationLayers = {
        "VK_LAYER_KHRONOS_validation"
    };
#ifdef NDEBUG
    const bool enableValidationLayers = false;
#else
    const bool enableValidationLayers = true;
    createInfo.enabledLayerCount = static_cast<uint32_t>(validationLayers.size());
    createInfo.ppEnabledLayerNames = validationLayers.data();
#endif

    // Optional: Specify required extensions
    std::vector<const char*> extensions = {
        // Add necessary extensions here, e.g., "VK_KHR_surface", "VK_KHR_win32_surface" for Windows
    };
    createInfo.enabledExtensionCount = static_cast<uint32_t>(extensions.size());
    createInfo.ppEnabledExtensionNames = extensions.data();

    VkResult result = vkCreateInstance(&createInfo, nullptr, &instance);
    if (result != VK_SUCCESS) {
        throw std::runtime_error("Failed to create Vulkan instance.");
    }

    spdlog::info("Vulkan instance created.");

    // 2. Pick Physical Device
    pickPhysicalDevice();

    spdlog::info("Selected Vulkan physical device.");

    // 3. Find Compute Queue Family Index
    findComputeQueueFamily();

    spdlog::info("Found compute queue family index: {}", computeQueueFamilyIndex);

    // 4. Create Logical Device and Retrieve Queues
    createLogicalDevice();

    spdlog::info("Logical device created and compute queue retrieved.");

    // 5. Create Command Pool
    VkCommandPoolCreateInfo poolInfo = {};
    poolInfo.sType = VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO;
    poolInfo.queueFamilyIndex = computeQueueFamilyIndex;
    poolInfo.flags = VK_COMMAND_POOL_CREATE_RESET_COMMAND_BUFFER_BIT; // Allow resetting individual command buffers

    result = vkCreateCommandPool(device, &poolInfo, nullptr, &vulkan_globals::commandPool);
    if (result != VK_SUCCESS) {
        throw std::runtime_error("Failed to create command pool.");
    }

    spdlog::info("Command pool created.");

    // 6. Create Descriptor Pool
    std::vector<VkDescriptorPoolSize> poolSizes = {
        { VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1000 } // Adjust as needed
    };

    VkDescriptorPoolCreateInfo poolCreateInfo = {};
    poolCreateInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO;
    poolCreateInfo.poolSizeCount = static_cast<uint32_t>(poolSizes.size());
    poolCreateInfo.pPoolSizes = poolSizes.data();
    poolCreateInfo.maxSets = 1000; // Adjust as needed

    result = vkCreateDescriptorPool(device, &poolCreateInfo, nullptr, &vulkan_globals::descriptorPool);
    if (result != VK_SUCCESS) {
        throw std::runtime_error("Failed to create descriptor pool.");
    }

    spdlog::info("Descriptor pool created.");

    // 7. Create Descriptor Set Layout
    std::vector<VkDescriptorSetLayoutBinding> layoutBindings = {
        // Example binding; adjust based on shader requirements
        {0, VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1, VK_SHADER_STAGE_COMPUTE_BIT, nullptr},
        {1, VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1, VK_SHADER_STAGE_COMPUTE_BIT, nullptr},
        {2, VK_DESCRIPTOR_TYPE_STORAGE_BUFFER, 1, VK_SHADER_STAGE_COMPUTE_BIT, nullptr} // Output buffer
    };

    VkDescriptorSetLayoutCreateInfo layoutInfo = {};
    layoutInfo.sType = VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO;
    layoutInfo.bindingCount = static_cast<uint32_t>(layoutBindings.size());
    layoutInfo.pBindings = layoutBindings.data();

    VkDescriptorSetLayout descriptorSetLayout;
    result = vkCreateDescriptorSetLayout(device, &layoutInfo, nullptr, &descriptorSetLayout);
    if (result != VK_SUCCESS) {
        throw std::runtime_error("Failed to create descriptor set layout.");
    }

    spdlog::info("Descriptor set layout created.");

    // 8. Initialize Managers
    memoryManager = std::make_unique<VulkanMemoryManager>(physicalDevice, device);
    bufferPool = std::make_unique<VulkanBufferPool>(device);
    
    // Initialize ShaderManager
    std::shared_ptr<ShaderManager> shaderMgr = std::make_shared<ShaderManager>(device);
    
    // Initialize PipelineManager
    pipelineManager = std::make_unique<PipelineManager>(shaderMgr, device);
    
    // Initialize CommandBufferManager
    commandBufferManager = std::make_unique<CommandBufferManager>(device, vulkan_globals::commandPool);
    
    // Initialize DescriptorSetManager with the created layout
    descriptorSetManager = std::make_shared<DescriptorSetManager>(device, vulkan_globals::descriptorPool, descriptorSetLayout);

    spdlog::info("CommandBufferManager and DescriptorSetManager created.");

    // Assign the Vulkan device and compute queue to the global variables
    vulkan_globals::device = device;
    vulkan_globals::computeQueue = computeQueue;

    spdlog::info("VulkanContext fully initialized.");
    spdlog::info("VulkanContext initialized successfully.");
}

// Cleanup Vulkan resources
void VulkanContext::cleanupVulkan() {
    if (device != VK_NULL_HANDLE) {
        spdlog::info("Cleaning up VulkanContext...");

        // Cleanup managers in reverse order of creation
        descriptorSetManager.reset();
        commandBufferManager.reset();
        pipelineManager.reset();
        bufferPool.reset();
        memoryManager.reset();

        // Destroy Descriptor Set Layout
        // Assuming DescriptorSetManager handles its own destruction

        vkDestroyDescriptorPool(device, vulkan_globals::descriptorPool, nullptr);
        vkDestroyCommandPool(device, vulkan_globals::commandPool, nullptr);

        vkDestroyDevice(device, nullptr);
        device = VK_NULL_HANDLE;

        spdlog::info("Logical device destroyed.");
    }

    if (instance != VK_NULL_HANDLE) {
        vkDestroyInstance(instance, nullptr);
        instance = VK_NULL_HANDLE;

        spdlog::info("Vulkan instance destroyed.");
    }

    // Reset global device and compute queue variables
    vulkan_globals::device = VK_NULL_HANDLE;
    vulkan_globals::computeQueue = VK_NULL_HANDLE;

    spdlog::info("VulkanContext cleaned up.");
}

// Pick a suitable physical device
void VulkanContext::pickPhysicalDevice() {
    uint32_t deviceCount = 0;
    vkEnumeratePhysicalDevices(instance, &deviceCount, nullptr);

    if (deviceCount == 0) {
        throw std::runtime_error("Failed to find GPUs with Vulkan support.");
    }

    std::vector<VkPhysicalDevice> devices(deviceCount);
    vkEnumeratePhysicalDevices(instance, &deviceCount, devices.data());

    // Select the first suitable device
    for (const auto& dev : devices) {
        // Here, you can add more checks to select a better device based on criteria like Vulkan version, extensions, etc.
        physicalDevice = dev;
        break;
    }

    if (physicalDevice == VK_NULL_HANDLE) {
        throw std::runtime_error("Failed to find a suitable GPU.");
    }
}

// Find a queue family that supports compute operations
void VulkanContext::findComputeQueueFamily() {
    uint32_t queueFamilyCount = 0;
    vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &queueFamilyCount, nullptr);

    if (queueFamilyCount == 0) {
        throw std::runtime_error("Failed to find any queue families.");
    }

    std::vector<VkQueueFamilyProperties> queueFamilies(queueFamilyCount);
    vkGetPhysicalDeviceQueueFamilyProperties(physicalDevice, &queueFamilyCount, queueFamilies.data());

    // Look for a queue family that supports compute
    for (uint32_t i = 0; i < queueFamilies.size(); ++i) {
        if (queueFamilies[i].queueFlags & VK_QUEUE_COMPUTE_BIT) {
            computeQueueFamilyIndex = i;
            computeQueue = VK_NULL_HANDLE; // Will be retrieved after device creation
            return;
        }
    }

    throw std::runtime_error("Failed to find a compute queue family.");
}

// Create a logical device and retrieve compute queue
void VulkanContext::createLogicalDevice() {
    float queuePriority = 1.0f;
    VkDeviceQueueCreateInfo queueCreateInfo = {};
    queueCreateInfo.sType = VK_STRUCTURE_TYPE_DEVICE_QUEUE_CREATE_INFO;
    queueCreateInfo.queueFamilyIndex = computeQueueFamilyIndex;
    queueCreateInfo.queueCount = 1;
    queueCreateInfo.pQueuePriorities = &queuePriority;

    // Specify device features if needed
    VkPhysicalDeviceFeatures deviceFeatures = {};
    deviceFeatures.shaderStorageImageExtendedFormats = VK_TRUE; // Example feature

    VkDeviceCreateInfo createInfo = {};
    createInfo.sType = VK_STRUCTURE_TYPE_DEVICE_CREATE_INFO;

    createInfo.pQueueCreateInfos = &queueCreateInfo;
    createInfo.queueCreateInfoCount = 1;

    createInfo.pEnabledFeatures = &deviceFeatures;

    // Enable necessary device extensions
    std::vector<const char*> deviceExtensions = {
        // Add required device extensions here, e.g., "VK_KHR_swapchain" if needed
    };
    createInfo.enabledExtensionCount = static_cast<uint32_t>(deviceExtensions.size());
    createInfo.ppEnabledExtensionNames = deviceExtensions.data();

    // Optional: Enable validation layers (if not already enabled at instance level)
    std::vector<const char*> validationLayers = {
        "VK_LAYER_KHRONOS_validation"
    };
#ifndef NDEBUG
    createInfo.enabledLayerCount = static_cast<uint32_t>(validationLayers.size());
    createInfo.ppEnabledLayerNames = validationLayers.data();
#else
    createInfo.enabledLayerCount = 0;
#endif

    VkResult result = vkCreateDevice(physicalDevice, &createInfo, nullptr, &device);
    if (result != VK_SUCCESS) {
        throw std::runtime_error("Failed to create logical device.");
    }

    // Retrieve the compute queue
    vkGetDeviceQueue(device, computeQueueFamilyIndex, 0, &computeQueue);
    if (computeQueue == VK_NULL_HANDLE) {
        throw std::runtime_error("Failed to retrieve compute queue.");
    }

    spdlog::info("Compute queue retrieved successfully.");
}

// src\VulkanMemoryManager.cpp
// src\VulkanMemoryManager.cpp
#include "VulkanMemoryManager.h"

VulkanMemoryManager::VulkanMemoryManager(VkPhysicalDevice physicalDevice, VkDevice device)
    : physicalDevice(physicalDevice), device(device) {}

VulkanMemoryManager::~VulkanMemoryManager() {
    for (auto& allocation : allocations) {
        if (allocation.memory != VK_NULL_HANDLE) {
            vkFreeMemory(device, allocation.memory, nullptr);
        }
    }
}

VulkanMemoryManager::AllocationInfo VulkanMemoryManager::allocate(VkDeviceSize size, VkMemoryPropertyFlags properties) {
    // Create a temporary buffer to get memory requirements
    VkBufferCreateInfo bufferInfo = {};
    bufferInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
    bufferInfo.size = size;
    bufferInfo.usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | 
                      VK_BUFFER_USAGE_TRANSFER_SRC_BIT | 
                      VK_BUFFER_USAGE_TRANSFER_DST_BIT;
    bufferInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;

    VkBuffer tempBuffer;
    VkResult result = vkCreateBuffer(device, &bufferInfo, nullptr, &tempBuffer);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to create temporary buffer for memory allocation.");
    }

    // Get memory requirements for the buffer
    VkMemoryRequirements memRequirements;
    vkGetBufferMemoryRequirements(device, tempBuffer, &memRequirements);
    vkDestroyBuffer(device, tempBuffer, nullptr);

    // Find suitable memory type
    uint32_t memoryTypeIndex = findMemoryType(memRequirements.memoryTypeBits, properties);
    if (memoryTypeIndex == std::numeric_limits<uint32_t>::max()) {
        throw VulkanError("Failed to find suitable memory type.");
    }

    // Allocate memory with proper alignment
    VkMemoryAllocateInfo allocInfo = {};
    allocInfo.sType = VK_STRUCTURE_TYPE_MEMORY_ALLOCATE_INFO;
    allocInfo.allocationSize = memRequirements.size;  // Use the size from requirements
    allocInfo.memoryTypeIndex = memoryTypeIndex;

    AllocationInfo allocation = {};
    result = vkAllocateMemory(device, &allocInfo, nullptr, &allocation.memory);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to allocate device memory.");
    }

    allocation.size = memRequirements.size;
    allocation.offset = 0;

    // Store allocation for cleanup
    allocations.push_back(allocation);

    return allocation;
}

uint32_t VulkanMemoryManager::findMemoryType(uint32_t typeFilter, VkMemoryPropertyFlags properties) {
    VkPhysicalDeviceMemoryProperties memProperties;
    vkGetPhysicalDeviceMemoryProperties(physicalDevice, &memProperties);

    // First try to find an exactly matching memory type
    for (uint32_t i = 0; i < memProperties.memoryTypeCount; i++) {
        if ((typeFilter & (1 << i)) && 
            (memProperties.memoryTypes[i].propertyFlags & properties) == properties) {
            return i;
        }
    }

    // If exact match not found, try to find a memory type with additional properties
    for (uint32_t i = 0; i < memProperties.memoryTypeCount; i++) {
        if ((typeFilter & (1 << i)) && 
            (memProperties.memoryTypes[i].propertyFlags & properties)) {
            return i;
        }
    }

    return std::numeric_limits<uint32_t>::max();
}

// src\VulkanOperations.cpp
// src\VulkanOperations.cpp
#include "VulkanOperations.h"
#include "vulkan_globals.h"
#include "VulkanContext.h"
#include "VulkanSync.h"
#include "DescriptorSetManager.h"
#include "VulkanTensor.h"
#include <spdlog/spdlog.h>

namespace vulkan_ops {

template<typename PushConstants>
void executeShader(
    VulkanOperationType opType,
    const VulkanTensor& inputA,
    const VulkanTensor* inputB,
    const VulkanTensor* inputC,
    VulkanTensor& output,
    const PushConstants* pushConstants
) {
    spdlog::debug("Executing shader for operation: {}", static_cast<int>(opType));

    VulkanContext* context = vulkan_globals::getContext();
    if (!context) {
        throw VulkanError("Vulkan context is null.");
    }

    // Validate tensors
    if (!inputA.isValid() || !output.isValid()) {
        throw VulkanError("Invalid input or output tensor.");
    }
    if (inputB && !inputB->isValid()) {
        throw VulkanError("Invalid inputB tensor.");
    }
    if (inputC && !inputC->isValid()) {
        throw VulkanError("Invalid inputC tensor.");
    }

    // Create pipeline key
    PipelineKey key = {
        opType,
        {}, // Input formats can be populated if needed
        {}, // Output formats can be populated if needed
        256, 1, 1 // Default workgroup sizes; adjust as per shader requirements
    };

    // Get pipeline and layout
    VkPipeline pipeline = context->getPipelineManager()->getPipeline(key);
    VkPipelineLayout pipelineLayout = context->getPipelineManager()->getPipelineLayout(key);
    
    if (pipeline == VK_NULL_HANDLE || pipelineLayout == VK_NULL_HANDLE) {
        throw VulkanError("Failed to get pipeline or pipeline layout.");
    }

    // Get command buffer
    VkCommandBuffer commandBuffer = context->getCommandBufferManager()->acquireCommandBuffer();

    // Begin command buffer
    VkCommandBufferBeginInfo beginInfo = {};
    beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    beginInfo.flags = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;

    if (vkBeginCommandBuffer(commandBuffer, &beginInfo) != VK_SUCCESS) {
        throw VulkanError("Failed to begin command buffer.");
    }

    // Memory barrier before execution
    VkMemoryBarrier preExecutionBarrier = {};
    preExecutionBarrier.sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER;
    preExecutionBarrier.srcAccessMask = VK_ACCESS_HOST_WRITE_BIT;
    preExecutionBarrier.dstAccessMask = VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT;

    vkCmdPipelineBarrier(
        commandBuffer,
        VK_PIPELINE_STAGE_HOST_BIT,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        0,
        1, &preExecutionBarrier,
        0, nullptr,
        0, nullptr
    );

    // Bind pipeline
    vkCmdBindPipeline(commandBuffer, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);

    // Update and bind descriptor sets
    auto descriptorManager = context->getDescriptorSetManager();
    VkDescriptorSet descriptorSet = descriptorManager->allocateDescriptorSet();

    std::vector<VkBuffer> buffers = { inputA.getBuffer() };
    if (inputB) buffers.push_back(inputB->getBuffer());
    if (inputC) buffers.push_back(inputC->getBuffer());
    descriptorManager->updateDescriptorSet(descriptorSet, buffers, output.getBuffer());

    vkCmdBindDescriptorSets(
        commandBuffer,
        VK_PIPELINE_BIND_POINT_COMPUTE,
        pipelineLayout,
        0,
        1,
        &descriptorSet,
        0,
        nullptr
    );

    // Push constants if provided
    if (pushConstants) {
        vkCmdPushConstants(
            commandBuffer,
            pipelineLayout,
            VK_SHADER_STAGE_COMPUTE_BIT,
            0,
            sizeof(PushConstants),
            pushConstants
        );
    }

    // Calculate dispatch dimensions based on workgroup size and output tensor dimensions
    uint32_t globalWorkSizeX = (output.getWidth() + key.workgroupSizeX - 1) / key.workgroupSizeX;
    uint32_t globalWorkSizeY = (output.getHeight() + key.workgroupSizeY - 1) / key.workgroupSizeY;
    uint32_t globalWorkSizeZ = (output.getChannels() + key.workgroupSizeZ - 1) / key.workgroupSizeZ;

    // Ensure at least one workgroup is dispatched
    globalWorkSizeX = std::max(1u, globalWorkSizeX);
    globalWorkSizeY = std::max(1u, globalWorkSizeY);
    globalWorkSizeZ = std::max(1u, globalWorkSizeZ);

    // Dispatch compute shader
    vkCmdDispatch(commandBuffer, globalWorkSizeX, globalWorkSizeY, globalWorkSizeZ);

    // Memory barrier after execution
    VkMemoryBarrier postExecutionBarrier = {};
    postExecutionBarrier.sType = VK_STRUCTURE_TYPE_MEMORY_BARRIER;
    postExecutionBarrier.srcAccessMask = VK_ACCESS_SHADER_WRITE_BIT;
    postExecutionBarrier.dstAccessMask = VK_ACCESS_HOST_READ_BIT;

    vkCmdPipelineBarrier(
        commandBuffer,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        VK_PIPELINE_STAGE_HOST_BIT,
        0,
        1, &postExecutionBarrier,
        0, nullptr,
        0, nullptr
    );

    // End command buffer
    if (vkEndCommandBuffer(commandBuffer) != VK_SUCCESS) {
        throw VulkanError("Failed to end command buffer.");
    }

    // Submit command buffer with fence
    VulkanSync::ScopedGPUWait scopedWait(deviceRef);
    
    VkSubmitInfo submitInfo = {};
    submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    submitInfo.commandBufferCount = 1;
    submitInfo.pCommandBuffers = &commandBuffer;

    if (vkQueueSubmit(vulkan_globals::computeQueue, 1, &submitInfo, scopedWait.get()) != VK_SUCCESS) {
        throw VulkanError("Failed to submit command buffer.");
    }

    // Wait for completion
    scopedWait.wait();

    // Release command buffer
    context->getCommandBufferManager()->releaseCommandBuffer(commandBuffer);
}

void executeAdd(const VulkanTensor& inputA, const VulkanTensor& inputB, VulkanTensor& output) {
    if (inputA.getSize() != inputB.getSize() || inputA.getSize() != output.getSize()) {
        throw std::runtime_error("Tensor size mismatch in Add operation.");
    }

    AddPushConstants pushConstants = { static_cast<uint32_t>(inputA.getSize() / sizeof(float)) };
    executeShader(VulkanOperationType::Add, inputA, &inputB, nullptr, output, &pushConstants);
}

void executeMatMul(const VulkanTensor& a, const VulkanTensor& b, VulkanTensor& c, 
                   uint32_t M, uint32_t K, uint32_t N) {
    if (a.getSize() != M * K * sizeof(float) ||
        b.getSize() != K * N * sizeof(float) ||
        c.getSize() != M * N * sizeof(float)) {
        throw std::runtime_error("Tensor dimensions mismatch in MatMul operation.");
    }

    MatMulPushConstants pushConstants = { M, K, N };
    executeShader(VulkanOperationType::MatMul, a, &b, nullptr, c, &pushConstants);
}

void executeReLU(const VulkanTensor& input, VulkanTensor& output) {
    if (input.getSize() != output.getSize()) {
        throw std::runtime_error("Tensor size mismatch in ReLU operation.");
    }

    ReLUPushConstants pushConstants = { static_cast<uint32_t>(input.getSize() / sizeof(float)) };
    executeShader(VulkanOperationType::ReLU, input, nullptr, nullptr, output, &pushConstants);
}

void executeSigmoid(const VulkanTensor& input, VulkanTensor& output) {
    if (input.getSize() != output.getSize()) {
        throw std::runtime_error("Tensor size mismatch in Sigmoid operation.");
    }

    SigmoidPushConstants pushConstants = { static_cast<uint32_t>(input.getSize() / sizeof(float)) };
    executeShader(VulkanOperationType::Sigmoid, input, nullptr, nullptr, output, &pushConstants);
}

void executeSoftmax(const VulkanTensor& input, VulkanTensor& output) {
    if (input.getSize() != output.getSize()) {
        throw std::runtime_error("Tensor size mismatch in Softmax operation.");
    }

    SoftmaxPushConstants pushConstants = { static_cast<uint32_t>(input.getSize() / sizeof(float)) };
    executeShader(VulkanOperationType::Softmax, input, nullptr, nullptr, output, &pushConstants);
}

void executeConv2D(const VulkanTensor& input, const VulkanTensor& kernel, 
                   VulkanTensor& output, const Conv2DPushConstants& pushConstants) {
    uint32_t expectedOutputWidth = 
        (pushConstants.input_width + 2 * pushConstants.padding - pushConstants.kernel_size) / 
        pushConstants.stride + 1;
    uint32_t expectedOutputHeight = 
        (pushConstants.input_height + 2 * pushConstants.padding - pushConstants.kernel_size) / 
        pushConstants.stride + 1;

    if (output.getWidth() != expectedOutputWidth ||
        output.getHeight() != expectedOutputHeight ||
        output.getChannels() != pushConstants.output_channels) {
        throw std::runtime_error("Output tensor dimensions mismatch in Conv2D operation.");
    }

    executeShader(VulkanOperationType::Conv2D, input, &kernel, nullptr, output, &pushConstants);
}

void executeMaxPool(const VulkanTensor& input, VulkanTensor& output, 
                    uint32_t width, uint32_t height, uint32_t channels,
                    uint32_t poolSizeX, uint32_t poolSizeY, 
                    uint32_t strideX, uint32_t strideY) {
    uint32_t expectedOutputWidth = (width - poolSizeX) / strideX + 1;
    uint32_t expectedOutputHeight = (height - poolSizeY) / strideY + 1;

    if (output.getWidth() != expectedOutputWidth ||
        output.getHeight() != expectedOutputHeight ||
        output.getChannels() != channels) {
        throw std::runtime_error("Output tensor dimensions mismatch in MaxPool operation.");
    }

    MaxPoolPushConstants pushConstants = { width, height, channels, 1, poolSizeX, poolSizeY, strideX, strideY };
    executeShader(VulkanOperationType::MaxPool, input, nullptr, nullptr, output, &pushConstants);
}

void executeBatchNorm(const VulkanTensor& input, const VulkanTensor& gamma, 
                      const VulkanTensor& beta, VulkanTensor& output, 
                      uint32_t size, float epsilon) {
    if (input.getSize() != output.getSize() ||
        gamma.getSize() != size * sizeof(float) ||
        beta.getSize() != size * sizeof(float)) {
        throw std::runtime_error("Tensor dimensions mismatch in BatchNorm operation.");
    }

    BatchNormPushConstants pushConstants = { size, epsilon };
    executeShader(VulkanOperationType::BatchNorm, input, &gamma, &beta, output, &pushConstants);
}

} // namespace vulkan_ops

// Explicit template instantiation for PushConstants structs
namespace vulkan_ops {
    template void executeShader<AddPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const AddPushConstants* pushConstants
    );

    template void executeShader<MatMulPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const MatMulPushConstants* pushConstants
    );

    template void executeShader<ReLUPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const ReLUPushConstants* pushConstants
    );

    template void executeShader<SigmoidPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const SigmoidPushConstants* pushConstants
    );

    template void executeShader<SoftmaxPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const SoftmaxPushConstants* pushConstants
    );

    template void executeShader<Conv2DPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const Conv2DPushConstants* pushConstants
    );

    template void executeShader<MaxPoolPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const MaxPoolPushConstants* pushConstants
    );

    template void executeShader<BatchNormPushConstants>(
        VulkanOperationType opType,
        const VulkanTensor& inputA,
        const VulkanTensor* inputB,
        const VulkanTensor* inputC,
        VulkanTensor& output,
        const BatchNormPushConstants* pushConstants
    );
}

// src\VulkanSync.cpp
#include "VulkanSync.h"

ScopedGPUWait::ScopedGPUWait(VkDevice device) : deviceRef(device), fence(VK_NULL_HANDLE) {
    VkFenceCreateInfo fenceInfo = {};
    fenceInfo.sType = VK_STRUCTURE_TYPE_FENCE_CREATE_INFO;
    fenceInfo.flags = 0;

    VkResult result = vkCreateFence(deviceRef, &fenceInfo, nullptr, &fence);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to create fence during ScopedGPUWait.");
    }
}

ScopedGPUWait::~ScopedGPUWait() {
    if (fence != VK_NULL_HANDLE) {
        vkDestroyFence(deviceRef, fence, nullptr);
        fence = VK_NULL_HANDLE;
    }
}

void ScopedGPUWait::wait() const {
    if (fence == VK_NULL_HANDLE) {
        throw VulkanError("Attempting to wait on null fence.");
    }

    VkResult result = vkWaitForFences(deviceRef, 1, &fence, VK_TRUE, UINT64_MAX);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to wait for fence in ScopedGPUWait.");
    }

    result = vkResetFences(deviceRef, 1, &fence);
    if (result != VK_SUCCESS) {
        throw VulkanError("Failed to reset fence in ScopedGPUWait.");
    }
}

VkFence ScopedGPUWait::get() const {
    return fence;
}
// src\VulkanTensor.cpp
#include "VulkanTensor.h"
#include "VulkanDeviceHelper.h"
#include <spdlog/spdlog.h>

VulkanTensor::VulkanTensor()
    : allocation{VK_NULL_HANDLE, 0, 0}
    , dimensions{0, 0, 0, 0, TensorLayout::Layout::LINEAR}
    , buffer(VK_NULL_HANDLE)
    , bufferPoolPtr(nullptr)
    , deviceRef(vulkan_globals::device) 
{
    spdlog::debug("Created empty VulkanTensor");
}

VulkanTensor::VulkanTensor(
    VulkanMemoryManager* memoryManager,
    VulkanBufferPool* bufferPool,
    VkDeviceSize size,
    uint32_t w,
    uint32_t h,
    uint32_t c,
    uint32_t n,
    const void* data,
    TensorLayout::Layout layout)
    : dimensions{n, h, w, c, layout}
    , bufferPoolPtr(bufferPool)
    , deviceRef(vulkan_globals::device)
{
    if (deviceRef == VK_NULL_HANDLE) {
        throw std::runtime_error("Vulkan device not initialized.");
    }

    auto context = vulkan_globals::getContext();
    if (!context) {
        throw std::runtime_error("Vulkan context is null.");
    }

    // Get device properties for alignment
    VkPhysicalDeviceProperties deviceProperties = 
        VulkanDeviceHelper::getPhysicalDeviceProperties(context->getPhysicalDevice());
    
    // Ensure size alignment
    VkDeviceSize alignment = deviceProperties.limits.minStorageBufferOffsetAlignment;
    size = (size + alignment - 1) & ~(alignment - 1);

    // Allocate memory
    try {
        allocation = memoryManager->allocate(
            size,
            VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT
        );
    } catch (const std::exception& e) {
        throw std::runtime_error(std::string("Failed to allocate memory: ") + e.what());
    }

    // Create buffer
    VkBufferCreateInfo bufferInfo = {};
    bufferInfo.sType = VK_STRUCTURE_TYPE_BUFFER_CREATE_INFO;
    bufferInfo.size = size;
    bufferInfo.usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT |
                       VK_BUFFER_USAGE_TRANSFER_DST_BIT |
                       VK_BUFFER_USAGE_TRANSFER_SRC_BIT;
    bufferInfo.sharingMode = VK_SHARING_MODE_EXCLUSIVE;

    VkResult result = vkCreateBuffer(deviceRef, &bufferInfo, nullptr, &buffer);
    if (result != VK_SUCCESS) {
        throw std::runtime_error("Failed to create buffer in VulkanTensor.");
    }

    result = vkBindBufferMemory(deviceRef, buffer, allocation.memory, allocation.offset);
    if (result != VK_SUCCESS) {
        vkDestroyBuffer(deviceRef, buffer, nullptr);
        throw std::runtime_error("Failed to bind buffer memory in VulkanTensor.");
    }

    spdlog::debug("Created VulkanTensor with dimensions {}x{}x{}x{}", n, h, w, c);

    if (data) {
        upload(data);
    }
}

VulkanTensor::VulkanTensor(VulkanTensor&& other) noexcept
    : allocation(other.allocation)
    , dimensions(other.dimensions)
    , buffer(other.buffer)
    , bufferPoolPtr(other.bufferPoolPtr)
    , deviceRef(other.deviceRef)
{
    other.buffer = VK_NULL_HANDLE;
    other.bufferPoolPtr = nullptr;
    spdlog::debug("Moved VulkanTensor");
}

VulkanTensor& VulkanTensor::operator=(VulkanTensor&& other) noexcept {
    if (this != &other) {
        cleanup();

        allocation = other.allocation;
        dimensions = other.dimensions;
        buffer = other.buffer;
        bufferPoolPtr = other.bufferPoolPtr;
        deviceRef = other.deviceRef;

        other.buffer = VK_NULL_HANDLE;
        other.bufferPoolPtr = nullptr;
        spdlog::debug("Move assigned VulkanTensor");
    }
    return *this;
}

VulkanTensor::~VulkanTensor() {
    cleanup();
    spdlog::debug("Destroyed VulkanTensor");
}

void VulkanTensor::upload(const void* data) {
    if (!data) {
        throw std::runtime_error("Null pointer provided for upload.");
    }

    VulkanContext* context = vulkan_globals::getContext();
    if (!context) {
        throw std::runtime_error("Vulkan context is null.");
    }

    VkCommandBuffer cmdBuffer = context->getCommandBufferManager()->acquireCommandBuffer();
    
    VkCommandBufferBeginInfo beginInfo = {};
    beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    beginInfo.flags = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
    
    if (vkBeginCommandBuffer(cmdBuffer, &beginInfo) != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to begin command buffer during upload.");
    }

    // Add buffer memory barrier for upload
    VkBufferMemoryBarrier barrier = {};
    barrier.sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER;
    barrier.srcAccessMask = VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT;
    barrier.dstAccessMask = VK_ACCESS_HOST_WRITE_BIT;
    barrier.buffer = buffer;
    barrier.offset = 0;
    barrier.size = allocation.size;
    barrier.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
    barrier.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;

    vkCmdPipelineBarrier(
        cmdBuffer,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        VK_PIPELINE_STAGE_HOST_BIT,
        0,
        0, nullptr,
        1, &barrier,
        0, nullptr
    );

    if (vkEndCommandBuffer(cmdBuffer) != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to end command buffer during upload.");
    }

    // Submit and wait
    VulkanSync::ScopedGPUWait scopedWait(deviceRef);
    VkSubmitInfo submitInfo = {};
    submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    submitInfo.commandBufferCount = 1;
    submitInfo.pCommandBuffers = &cmdBuffer;

    if (vkQueueSubmit(vulkan_globals::computeQueue, 1, &submitInfo, scopedWait.get()) != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to submit command buffer during upload.");
    }

    scopedWait.wait();

    // Map and copy data
    void* mappedMemory;
    VkResult result = vkMapMemory(deviceRef, allocation.memory, allocation.offset, allocation.size, 0, &mappedMemory);
    if (result != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to map memory for upload.");
    }

    memcpy(mappedMemory, data, static_cast<size_t>(allocation.size));

    VkMappedMemoryRange memoryRange = {};
    memoryRange.sType = VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE;
    memoryRange.memory = allocation.memory;
    memoryRange.offset = allocation.offset;
    memoryRange.size = allocation.size;
    vkFlushMappedMemoryRanges(deviceRef, 1, &memoryRange);

    vkUnmapMemory(deviceRef, allocation.memory);
    context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);

    spdlog::debug("Uploaded data to VulkanTensor");
}

void VulkanTensor::download(void* data) const {
    if (!data) {
        throw std::runtime_error("Null pointer provided for download.");
    }

    VulkanContext* context = vulkan_globals::getContext();
    if (!context) {
        throw std::runtime_error("Vulkan context is null.");
    }

    VkCommandBuffer cmdBuffer = context->getCommandBufferManager()->acquireCommandBuffer();
    
    VkCommandBufferBeginInfo beginInfo = {};
    beginInfo.sType = VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO;
    beginInfo.flags = VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT;
    
    if (vkBeginCommandBuffer(cmdBuffer, &beginInfo) != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to begin command buffer during download.");
    }

    // Add buffer memory barrier for download
    VkBufferMemoryBarrier barrier = {};
    barrier.sType = VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER;
    barrier.srcAccessMask = VK_ACCESS_SHADER_WRITE_BIT;
    barrier.dstAccessMask = VK_ACCESS_HOST_READ_BIT;
    barrier.buffer = buffer;
    barrier.offset = 0;
    barrier.size = allocation.size;
    barrier.srcQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;
    barrier.dstQueueFamilyIndex = VK_QUEUE_FAMILY_IGNORED;

    vkCmdPipelineBarrier(
        cmdBuffer,
        VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
        VK_PIPELINE_STAGE_HOST_BIT,
        0,
        0, nullptr,
        1, &barrier,
        0, nullptr
    );

    if (vkEndCommandBuffer(cmdBuffer) != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to end command buffer during download.");
    }

    // Submit and wait
    VulkanSync::ScopedGPUWait scopedWait(deviceRef);
    VkSubmitInfo submitInfo = {};
    submitInfo.sType = VK_STRUCTURE_TYPE_SUBMIT_INFO;
    submitInfo.commandBufferCount = 1;
    submitInfo.pCommandBuffers = &cmdBuffer;

    if (vkQueueSubmit(vulkan_globals::computeQueue, 1, &submitInfo, scopedWait.get()) != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to submit command buffer during download.");
    }

    scopedWait.wait();

    // Map and copy data
    void* mappedMemory;
    VkResult result = vkMapMemory(deviceRef, allocation.memory, allocation.offset, allocation.size, 0, &mappedMemory);
    if (result != VK_SUCCESS) {
        context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
        throw std::runtime_error("Failed to map memory for download.");
    }

    VkMappedMemoryRange memoryRange = {};
    memoryRange.sType = VK_STRUCTURE_TYPE_MAPPED_MEMORY_RANGE;
    memoryRange.memory = allocation.memory;
    memoryRange.offset = allocation.offset;
    memoryRange.size = allocation.size;
    vkInvalidateMappedMemoryRanges(deviceRef, 1, &memoryRange);

    memcpy(data, mappedMemory, static_cast<size_t>(allocation.size));
    vkUnmapMemory(deviceRef, allocation.memory);

    context->getCommandBufferManager()->releaseCommandBuffer(cmdBuffer);
    spdlog::debug("Downloaded data from VulkanTensor");
}

uint32_t VulkanTensor::getN() const { return dimensions.n; }
uint32_t VulkanTensor::getC() const { return dimensions.c; }
uint32_t VulkanTensor::getH() const { return dimensions.h; }
uint32_t VulkanTensor::getW() const { return dimensions.w; }
uint32_t VulkanTensor::getWidth() const { return dimensions.w; }
uint32_t VulkanTensor::getHeight() const { return dimensions.h; }
uint32_t VulkanTensor::getChannels() const { return dimensions.c; }
VkDeviceSize VulkanTensor::getSize() const { return allocation.size; }
VkBuffer VulkanTensor::getBuffer() const { return buffer; }

TensorLayout::Layout VulkanTensor::getLayout() const { return dimensions.layout; }
void VulkanTensor::setLayout(TensorLayout::Layout newLayout) { dimensions.layout = newLayout; }

bool VulkanTensor::isValid() const {
    return buffer != VK_NULL_HANDLE && allocation.memory != VK_NULL_HANDLE;
}

std::string VulkanTensor::getDimensionsString() const {
    return "(" + std::to_string(dimensions.n) + ", " +
           std::to_string(dimensions.c) + ", " +
           std::to_string(dimensions.h) + ", " +
           std::to_string(dimensions.w) + ")";
}

bool VulkanTensor::verifyDimensions(VkDeviceSize expectedSize) const {
    return allocation.size == expectedSize &&
           (dimensions.w * dimensions.h * dimensions.c * dimensions.n * sizeof(float)) == expectedSize;
}

void VulkanTensor::debugPrint() const {
    std::vector<float> data(allocation.size / sizeof(float));
    download(data.data());
    std::cout << "Tensor data (first 10 elements): ";
    for (size_t i = 0; i < std::min<size_t>(10, data.size()); ++i) {
        std::cout << data[i] << " ";
    }
    std::cout << std::endl;
}

void VulkanTensor::cleanup() {
    if (buffer != VK_NULL_HANDLE) {
        if (bufferPoolPtr) {
            bufferPoolPtr->releaseBuffer(buffer);
        }
        vkDestroyBuffer(deviceRef, buffer, nullptr);
        buffer = VK_NULL_HANDLE;
        spdlog::debug("Cleaned up VulkanTensor resources");
    }
}
// src\vulkan_backend_bindings.cpp
// src\vulkan_backend_bindings.cpp
#include <pybind11/pybind11.h>
#include <pybind11/numpy.h>
#include <pybind11/stl.h>
#include <spdlog/spdlog.h>
#include <filesystem>
#include <stdexcept>
#include <vector>
#include <cstdint> // For int64_t

// Include your Vulkan backend headers
#include "VulkanOperations.h"    // Contains executeAdd, executeMatMul, etc.
#include "VulkanContext.h"       // Manages Vulkan context
#include "VulkanTensor.h"        // VulkanTensor class definition
#include "vulkan_globals.h"      // Global Vulkan context management
#include "OnnxModelParser.h"     // ONNX model parser
#include "Utils.h"               // For checkSize function and utility macros

namespace py = pybind11;

// -----------------------------
// Binding TensorLayout::Layout Enum
// -----------------------------
PYBIND11_MODULE(vulkan_backend, m) {
    m.doc() = "Vulkan Backend for PyTorch Operations";

    // Bind the TensorLayout::Layout enum to Python
    py::enum_<TensorLayout::Layout>(m, "Layout")
        .value("NHWC", TensorLayout::Layout::NHWC, "Batch, Height, Width, Channels")
        .value("NCHW", TensorLayout::Layout::NCHW, "Batch, Channels, Height, Width")
        .value("LINEAR", TensorLayout::Layout::LINEAR, "Flat layout")
        .export_values();

    // -----------------------------
    // Vulkan Initialization
    // -----------------------------
    m.def("init_vulkan", []() -> bool {
        try {
            // Get the path to the current module
            auto main_module = py::module::import("__main__");
            std::string main_file = main_module.attr("__file__").cast<std::string>();
            std::filesystem::path module_path = std::filesystem::path(main_file).parent_path();

            // Initialize shader directory based on module location
            vulkan_globals::setShaderDirectory(module_path);

            // Initialize Vulkan context
            bool success = vulkan_globals::initializeVulkan();
            if (success && vulkan_globals::getContext()) {
                vulkan_globals::device = vulkan_globals::getContext()->getDevice();
                spdlog::info("Vulkan initialized successfully.");
                return true;
            }
            spdlog::error("Vulkan initialization failed.");
            return false;
        }
        catch (const std::exception& e) {
            spdlog::error("Failed to initialize Vulkan: {}", e.what());
            return false;
        }
    }, "Initialize Vulkan context");

    // -----------------------------
    // Vulkan Cleanup
    // -----------------------------
    m.def("cleanup_vulkan", []() -> void {
        try {
            vulkan_globals::cleanupVulkan();
            vulkan_globals::device = VK_NULL_HANDLE;
            spdlog::info("Vulkan cleaned up successfully.");
        }
        catch (const std::exception& e) {
            spdlog::error("Failed to cleanup Vulkan: {}", e.what());
            throw std::runtime_error("Vulkan cleanup failed.");
        }
    }, "Cleanup Vulkan context");

    // -----------------------------
    // Check Vulkan Initialization
    // -----------------------------
    m.def("is_vulkan_initialized", []() -> bool {
        return vulkan_globals::getContext() != nullptr && vulkan_globals::device != VK_NULL_HANDLE;
    }, "Check if Vulkan context is initialized");

    // -----------------------------
    // Import ONNX Model
    // -----------------------------
    m.def("import_onnx_model", [](const std::string& modelPath) -> void {
        try {
            OnnxModelParser parser(modelPath);
            spdlog::info("ONNX model imported successfully: {}", modelPath);
        }
        catch (const std::exception& e) {
            spdlog::error("Failed to import ONNX model: {}", e.what());
            throw std::runtime_error("ONNX model import failed.");
        }
    }, "Import an ONNX model for execution on Vulkan backend");

    // -----------------------------
    // VulkanTensor Python Class
    // -----------------------------
    py::class_<VulkanTensor>(m, "VulkanTensor")
        .def(py::init<>(), "Default constructor for VulkanTensor.")
        .def_property("data", 
            // Getter: Download data from Vulkan buffer to NumPy array
            [](VulkanTensor &self) -> py::array_t<float> {
                // Get tensor dimensions and layout
                uint32_t n = self.getN();
                uint32_t c = self.getC();
                uint32_t h = self.getH();
                uint32_t w = self.getW();
                TensorLayout::Layout layout = self.getLayout();

                // Define shape based on layout
                std::vector<int64_t> shape;
                if (layout == TensorLayout::Layout::NHWC) {
                    shape = { static_cast<int64_t>(n), 
                              static_cast<int64_t>(h), 
                              static_cast<int64_t>(w), 
                              static_cast<int64_t>(c) };
                }
                else if (layout == TensorLayout::Layout::NCHW) {
                    shape = { static_cast<int64_t>(n), 
                              static_cast<int64_t>(c), 
                              static_cast<int64_t>(h), 
                              static_cast<int64_t>(w) };
                }
                else { // LINEAR
                    shape = { static_cast<int64_t>(n * c * h * w) };
                }

                // Allocate NumPy array without initializing data
                py::array_t<float> result(shape, nullptr);

                // Get pointer to NumPy data
                float* ptr = static_cast<float*>(result.request().ptr);

                // Download data from Vulkan buffer to host
                self.download(ptr);

                return result;
            },
            // Setter: Upload data from NumPy array to Vulkan buffer
            [](VulkanTensor &self, py::array_t<float> input) {
                // Ensure input array is contiguous
                if (!(input.flags() & py::array::c_style)) {
                    throw std::runtime_error("Input array must be contiguous.");
                }

                // Calculate expected size based on tensor dimensions
                size_t expected_size = static_cast<size_t>(self.getN()) *
                                       static_cast<size_t>(self.getC()) *
                                       static_cast<size_t>(self.getH()) *
                                       static_cast<size_t>(self.getW());
                if (input.size() != expected_size) {
                    throw std::runtime_error("Input array size does not match VulkanTensor size.");
                }

                // Get pointer to input data
                const float* ptr = static_cast<const float*>(input.request().ptr);

                // Upload data from host to Vulkan buffer
                self.upload(ptr);
            },
            "Data buffer of the tensor."
        )
        .def("get_shape", [](const VulkanTensor& self) -> std::vector<int64_t> {
            return { static_cast<int64_t>(self.getN()),
                     static_cast<int64_t>(self.getC()),
                     static_cast<int64_t>(self.getH()),
                     static_cast<int64_t>(self.getW()) };
        }, "Get the shape of the tensor.")
        .def("get_layout", &VulkanTensor::getLayout, "Get the tensor layout.")
        .def("set_layout", &VulkanTensor::setLayout, "Set the tensor layout.")
        .def("debug_print", &VulkanTensor::debugPrint, "Print first few elements of the tensor for debugging.");

    // -----------------------------
    // Factory Function to Create VulkanTensor
    // -----------------------------
    m.def("create_vulkan_tensor", [](py::array_t<float> data, 
                                     uint32_t n, 
                                     uint32_t h, 
                                     uint32_t w, 
                                     uint32_t c, 
                                     TensorLayout::Layout layout) -> VulkanTensor {
        if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
            throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
        }

        if (!(data.flags() & py::array::c_style)) {
            throw std::runtime_error("Input array must be contiguous.");
        }

        size_t expected_size = static_cast<size_t>(n) * h * w * c;
        if (data.size() != expected_size) {
            throw std::runtime_error("Input array size does not match specified dimensions.");
        }

        auto buf = data.request();
        const float* ptr = static_cast<const float*>(buf.ptr);

        auto context = vulkan_globals::getContext();
        VulkanTensor tensor(
            context->getMemoryManager(),
            context->getBufferPool(),
            checkSize(buf.size * sizeof(float)),
            w, h, c, n, // w, h, c, n
            ptr,
            layout
        );

        return tensor;
    }, 
    py::arg("data"),
    py::arg("n") = 1,
    py::arg("h") = 1,
    py::arg("w") = 1,
    py::arg("c") = 1,
    py::arg("layout") = TensorLayout::Layout::LINEAR,
    "Factory function to create VulkanTensor with data and dimensions.");

    // -----------------------------
    // Element-wise Addition Operation
    // -----------------------------
    m.def("vulkan_add", [](py::array_t<float> inputA, py::array_t<float> inputB, py::array_t<float> output) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input and output arrays are contiguous
            if (!(inputA.flags() & py::array::c_style) ||
                !(inputB.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input and output arrays must be contiguous.");
            }

            auto buf_a = inputA.request();
            auto buf_b = inputB.request();
            auto buf_c = output.request();

            // Ensure all arrays have the same size
            if (buf_a.size != buf_b.size || buf_a.size != buf_c.size) {
                throw std::runtime_error("Input and output arrays must have the same size.");
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorA(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_a.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                buf_a.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorB(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_b.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                buf_b.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorC(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_c.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                nullptr,
                TensorLayout::Layout::LINEAR
            );

            // Execute Add operation
            vulkan_ops::executeAdd(tensorA, tensorB, tensorC);

            // Download result
            tensorC.download(buf_c.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("Add operation failed: " + std::string(e.what()));
        }
    }, 
    py::arg("inputA"), py::arg("inputB"), py::arg("output"),
    "Execute element-wise addition on Vulkan backend");

    // -----------------------------
    // Matrix Multiplication Operation
    // -----------------------------
    m.def("vulkan_matmul", [](py::array_t<float> inputA, py::array_t<float> inputB, py::array_t<float> output,
                              uint32_t M, uint32_t K, uint32_t N) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input and output arrays are contiguous
            if (!(inputA.flags() & py::array::c_style) ||
                !(inputB.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input and output arrays must be contiguous.");
            }

            auto buf_a = inputA.request();
            auto buf_b = inputB.request();
            auto buf_c = output.request();

            // Calculate expected sizes
            size_t expected_size_a = static_cast<size_t>(M) * K;
            size_t expected_size_b = static_cast<size_t>(K) * N;
            size_t expected_size_c = static_cast<size_t>(M) * N;

            if (buf_a.size != expected_size_a ||
                buf_b.size != expected_size_b ||
                buf_c.size != expected_size_c) {
                throw std::runtime_error("Input and output array sizes do not match the specified dimensions.");
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorA(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_a.size * sizeof(float)),
                K, M, 1, 1, // w, h, c, n
                buf_a.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorB(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_b.size * sizeof(float)),
                N, K, 1, 1, // w, h, c, n
                buf_b.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorC(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_c.size * sizeof(float)),
                N, M, 1, 1, // w, h, c, n
                nullptr,
                TensorLayout::Layout::LINEAR
            );

            // Execute MatMul operation
            vulkan_ops::executeMatMul(tensorA, tensorB, tensorC, M, K, N);

            // Download result
            tensorC.download(buf_c.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("Matrix multiplication failed: " + std::string(e.what()));
        }
    }, 
    py::arg("inputA"), py::arg("inputB"), py::arg("output"), 
    py::arg("M"), py::arg("K"), py::arg("N"),
    "Execute matrix multiplication on Vulkan backend");

    // -----------------------------
    // ReLU Operation
    // -----------------------------
    m.def("vulkan_relu", [](py::array_t<float> input, py::array_t<float> output) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input and output arrays are contiguous
            if (!(input.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input and output arrays must be contiguous.");
            }

            auto buf_input = input.request();
            auto buf_output = output.request();

            // Ensure sizes match
            if (buf_input.size != buf_output.size) {
                throw std::runtime_error("Input and output arrays must have the same size.");
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorInput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_input.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                buf_input.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorOutput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_output.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                nullptr,
                TensorLayout::Layout::LINEAR
            );

            // Execute ReLU operation
            vulkan_ops::executeReLU(tensorInput, tensorOutput);

            // Download result
            tensorOutput.download(buf_output.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("ReLU operation failed: " + std::string(e.what()));
        }
    }, 
    py::arg("input"), py::arg("output"),
    "Execute ReLU activation on Vulkan backend");

    // -----------------------------
    // Sigmoid Operation
    // -----------------------------
    m.def("vulkan_sigmoid", [](py::array_t<float> input, py::array_t<float> output) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input and output arrays are contiguous
            if (!(input.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input and output arrays must be contiguous.");
            }

            auto buf_input = input.request();
            auto buf_output = output.request();

            // Ensure sizes match
            if (buf_input.size != buf_output.size) {
                throw std::runtime_error("Input and output arrays must have the same size.");
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorInput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_input.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                buf_input.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorOutput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_output.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                nullptr,
                TensorLayout::Layout::LINEAR
            );

            // Execute Sigmoid operation
            vulkan_ops::executeSigmoid(tensorInput, tensorOutput);

            // Download result
            tensorOutput.download(buf_output.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("Sigmoid operation failed: " + std::string(e.what()));
        }
    }, 
    py::arg("input"), py::arg("output"),
    "Execute Sigmoid activation on Vulkan backend");

    // -----------------------------
    // Softmax Operation
    // -----------------------------
    m.def("vulkan_softmax", [](py::array_t<float> input, py::array_t<float> output) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input and output arrays are contiguous
            if (!(input.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input and output arrays must be contiguous.");
            }

            auto buf_input = input.request();
            auto buf_output = output.request();

            // Ensure sizes match
            if (buf_input.size != buf_output.size) {
                throw std::runtime_error("Input and output arrays must have the same size.");
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorInput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_input.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                buf_input.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorOutput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_output.size * sizeof(float)),
                1, 1, 1, 1, // w, h, c, n
                nullptr,
                TensorLayout::Layout::LINEAR
            );

            // Execute Softmax operation
            vulkan_ops::executeSoftmax(tensorInput, tensorOutput);

            // Download result
            tensorOutput.download(buf_output.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("Softmax operation failed: " + std::string(e.what()));
        }
    }, 
    py::arg("input"), py::arg("output"),
    "Execute Softmax operation on Vulkan backend");

    // -----------------------------
    // Conv2D Operation
    // -----------------------------
    m.def("vulkan_conv2d", [](
        py::array_t<float> input,
        py::array_t<float> kernel,
        py::array_t<float> output,
        uint32_t input_width,
        uint32_t input_height,
        uint32_t input_channels,
        uint32_t output_channels,
        uint32_t kernel_size,
        uint32_t padding,
        uint32_t stride) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input, kernel, and output arrays are contiguous
            if (!(input.flags() & py::array::c_style) ||
                !(kernel.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input, kernel, and output arrays must be contiguous.");
            }

            auto buf_input = input.request();
            auto buf_kernel = kernel.request();
            auto buf_output = output.request();

            uint32_t output_width = (input_width + 2 * padding - kernel_size) / stride + 1;
            uint32_t output_height = (input_height + 2 * padding - kernel_size) / stride + 1;

            // Validate tensor sizes
            if (buf_input.size != static_cast<size_t>(input_width) * input_height * input_channels) {
                throw std::runtime_error("Input size mismatch.");
            }
            if (buf_kernel.size != static_cast<size_t>(kernel_size) * kernel_size * input_channels * output_channels) {
                throw std::runtime_error("Kernel size mismatch.");
            }
            if (buf_output.size != static_cast<size_t>(output_width) * output_height * output_channels) {
                throw std::runtime_error("Output size mismatch.");
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorInput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_input.size * sizeof(float)),
                input_width, input_height, input_channels, 1, // w, h, c, n
                buf_input.ptr,
                TensorLayout::Layout::NHWC
            );
            VulkanTensor tensorKernel(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_kernel.size * sizeof(float)),
                kernel_size, kernel_size, input_channels, output_channels, // w, h, c, n
                buf_kernel.ptr,
                TensorLayout::Layout::NHWC
            );
            VulkanTensor tensorOutput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_output.size * sizeof(float)),
                output_width, output_height, output_channels, 1, // w, h, c, n
                nullptr,
                TensorLayout::Layout::NHWC
            );

            // Prepare push constants
            Conv2DPushConstants pushConstants = {
                input_width,
                input_height,
                input_channels,
                output_channels,
                kernel_size,
                padding,
                stride
            };

            // Execute Conv2D operation
            vulkan_ops::executeConv2D(tensorInput, tensorKernel, tensorOutput, pushConstants);

            // Download results
            tensorOutput.download(buf_output.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("Conv2D operation failed: " + std::string(e.what()));
        }
    },
    py::arg("input"),
    py::arg("kernel"),
    py::arg("output"),
    py::arg("input_width"),
    py::arg("input_height"),
    py::arg("input_channels"),
    py::arg("output_channels"),
    py::arg("kernel_size"),
    py::arg("padding"),
    py::arg("stride"),
    "Execute 2D Convolution on Vulkan backend");

    // -----------------------------
    // MaxPool Operation
    // -----------------------------
    m.def("vulkan_maxpool", [](py::array_t<float> input, py::array_t<float> output,
                              uint32_t width, uint32_t height, uint32_t channels,
                              uint32_t poolSizeX, uint32_t poolSizeY, 
                              uint32_t strideX, uint32_t strideY) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input and output arrays are contiguous
            if (!(input.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input and output arrays must be contiguous.");
            }

            auto buf_input = input.request();
            auto buf_output = output.request();

            uint32_t expected_output_width = (width - poolSizeX) / strideX + 1;
            uint32_t expected_output_height = (height - poolSizeY) / strideY + 1;
            uint32_t expected_output_size = expected_output_width * expected_output_height * channels;

            if (buf_input.size != static_cast<size_t>(width) * height * channels) {
                throw std::runtime_error(
                    "Input dimensions mismatch. Expected size: " + std::to_string(width * height * channels) +
                    ", Got: " + std::to_string(buf_input.size)
                );
            }

            if (buf_output.size != expected_output_size) {
                throw std::runtime_error(
                    "Output dimensions mismatch. Expected size: " + std::to_string(expected_output_size) +
                    ", Got: " + std::to_string(buf_output.size)
                );
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorInput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_input.size * sizeof(float)),
                width, height, channels, 1, // w, h, c, n
                buf_input.ptr,
                TensorLayout::Layout::NHWC
            );
            VulkanTensor tensorOutput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_output.size * sizeof(float)),
                expected_output_width,
                expected_output_height,
                channels,
                1, // n
                nullptr,
                TensorLayout::Layout::NHWC
            );

            // Execute MaxPool operation
            vulkan_ops::executeMaxPool(tensorInput, tensorOutput, 
                                       width, height, channels,
                                       poolSizeX, poolSizeY, strideX, strideY);

            // Download result
            tensorOutput.download(buf_output.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("MaxPool operation failed: " + std::string(e.what()));
        }
    }, 
    py::arg("input"),
    py::arg("output"),
    py::arg("width"),
    py::arg("height"),
    py::arg("channels"),
    py::arg("poolSizeX"),
    py::arg("poolSizeY"),
    py::arg("strideX"),
    py::arg("strideY"),
    "Execute MaxPool2D operation on Vulkan backend");

    // -----------------------------
    // BatchNorm Operation
    // -----------------------------
    m.def("vulkan_batchnorm", [](
        py::array_t<float> input,
        py::array_t<float> gamma,
        py::array_t<float> beta,
        py::array_t<float> output,
        uint32_t size,
        float epsilon) {
        try {
            // Ensure Vulkan is initialized
            if (!vulkan_globals::getContext() || vulkan_globals::device == VK_NULL_HANDLE) {
                throw std::runtime_error("Vulkan is not initialized. Call init_vulkan() first.");
            }

            // Ensure input arrays are contiguous
            if (!(input.flags() & py::array::c_style) ||
                !(gamma.flags() & py::array::c_style) ||
                !(beta.flags() & py::array::c_style) ||
                !(output.flags() & py::array::c_style)) {
                throw std::runtime_error("Input, gamma, beta, and output arrays must be contiguous.");
            }

            auto buf_input = input.request();
            auto buf_gamma = gamma.request();
            auto buf_beta = beta.request();
            auto buf_output = output.request();

            if (buf_input.size != size ||
                buf_gamma.size != size ||
                buf_beta.size != size ||
                buf_output.size != size) {
                throw std::runtime_error("All tensors must have the specified size.");
            }

            auto context = vulkan_globals::getContext();

            // Create VulkanTensor instances
            VulkanTensor tensorInput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_input.size * sizeof(float)),
                1, 1, size, 1, // w, h, c, n
                buf_input.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorGamma(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_gamma.size * sizeof(float)),
                1, 1, size, 1, // w, h, c, n
                buf_gamma.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorBeta(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_beta.size * sizeof(float)),
                1, 1, size, 1, // w, h, c, n
                buf_beta.ptr,
                TensorLayout::Layout::LINEAR
            );
            VulkanTensor tensorOutput(
                context->getMemoryManager(),
                context->getBufferPool(),
                checkSize(buf_output.size * sizeof(float)),
                1, 1, size, 1, // w, h, c, n
                nullptr,
                TensorLayout::Layout::LINEAR
            );

            // Execute BatchNorm operation
            vulkan_ops::executeBatchNorm(tensorInput, tensorGamma, tensorBeta, tensorOutput, size, epsilon);

            // Download result
            tensorOutput.download(buf_output.ptr);
        }
        catch (const std::exception& e) {
            throw std::runtime_error("BatchNorm operation failed: " + std::string(e.what()));
        }
    }, 
    py::arg("input"),
    py::arg("gamma"),
    py::arg("beta"),
    py::arg("output"),
    py::arg("size"),
    py::arg("epsilon"),
    "Execute BatchNorm operation on Vulkan backend");

    // -----------------------------
    // Save Model Operation (Not Implemented)
    // -----------------------------
    m.def("save_model", [](const std::string& filename) -> void {
        throw std::runtime_error("Model saving not yet implemented.");
    }, "Save the current model to a file");

    // -----------------------------
    // Load Model Operation (Not Implemented)
    // -----------------------------
    m.def("load_model", [](const std::string& filename) -> void {
        throw std::runtime_error("Model loading not yet implemented.");
    }, "Load a model from a file");

    // -----------------------------
    // Initialize Distributed Training (Not Implemented)
    // -----------------------------
    m.def("initialize_distributed", [](uint32_t num_gpus) -> void {
        throw std::runtime_error("Distributed training not yet implemented.");
    }, "Initialize distributed training");

    // -----------------------------
    // Enable Gradient Checkpointing (Not Implemented)
    // -----------------------------
    m.def("enable_gradient_checkpointing", []() -> void {
        throw std::runtime_error("Gradient checkpointing not yet implemented.");
    }, "Enable gradient checkpointing");
}

// src\vulkan_globals.cpp
#include "vulkan_globals.h"
#include "VulkanContext.h"
#include "spdlog/spdlog.h"
#include <filesystem>

namespace vulkan_globals {
    // Define global Vulkan variables
    VkInstance instance = VK_NULL_HANDLE;
    VkPhysicalDevice physicalDevice = VK_NULL_HANDLE;
    VkDevice device = VK_NULL_HANDLE;
    VkQueue computeQueue = VK_NULL_HANDLE;
    VkQueue graphicsQueue = VK_NULL_HANDLE;
    VkCommandPool commandPool = VK_NULL_HANDLE;
    VkDescriptorPool descriptorPool = VK_NULL_HANDLE;

    // Initialize shader directory
    std::filesystem::path shader_directory;

    // Vulkan context instance
    std::unique_ptr<VulkanContext> vulkanContextInstance = nullptr;

    void setShaderDirectory(const std::filesystem::path& exe_path) {
        // Get the directory containing the executable
        auto base_path = exe_path.parent_path();

        // Potential shader paths
        std::vector<std::filesystem::path> potential_paths = {
            base_path / "shaders",                    // Direct shaders subdirectory
            base_path / "VulkanShaderCUDA" / "shaders", // In project directory
            base_path / ".." / "shaders",             // One level up
            base_path / ".." / "VulkanShaderCUDA" / "shaders", // One level up in project directory
            base_path / ".." / ".." / "shaders",      // Two levels up
            base_path / ".." / ".." / "VulkanShaderCUDA" / "shaders", // Two levels up in project directory
        };

        for (const auto& path : potential_paths) {
            if (std::filesystem::exists(path) && 
                std::filesystem::exists(path / "add.comp.spv")) { // Check for a key shader file
                shader_directory = path;
                spdlog::info("Found shader directory at: {}", shader_directory.string());
                return;
            }
        }

        spdlog::error("Could not find valid shader directory in any of the searched locations");
        for (const auto& path : potential_paths) {
            spdlog::error("Searched: {}", path.string());
        }

        // Throw exception if shader directory not found
        throw std::runtime_error("Could not find valid shader directory");
    }

    // Initialize Vulkan
    bool initializeVulkan() {
        try {
            if (!vulkanContextInstance) {
                spdlog::info("Creating VulkanContext instance...");
                vulkanContextInstance = std::make_unique<VulkanContext>();
                vulkanContextInstance->initVulkan();
                spdlog::info("VulkanContext initialized successfully.");
            } else {
                spdlog::warn("VulkanContext is already initialized.");
            }
            return true;
        }
        catch (const std::exception& e) {
            spdlog::error("Failed to initialize Vulkan: {}", e.what());
            return false;
        }
    }

    // Cleanup Vulkan
    void cleanupVulkan() {
        if (vulkanContextInstance) {
            spdlog::info("Cleaning up VulkanContext...");
            vulkanContextInstance->cleanupVulkan();
            vulkanContextInstance.reset();
            spdlog::info("VulkanContext cleanup complete.");
        } else {
            spdlog::warn("VulkanContext is not initialized or already cleaned up.");
        }
    }

    // Get VulkanContext instance
    VulkanContext* getContext() {
        return vulkanContextInstance.get();
    }
}

